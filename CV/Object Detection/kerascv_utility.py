#!/usr/bin/env python
# coding: utf-8
import os
import io
import pandas as pd
import tensorflow as tf
import numpy as np
import cv2
from sklearn import preprocessing
import keras

def xml_to_csv(path):
    """Iterates through all .xml files (generated by labelImg) in a given directory and 
    combines them in a single Pandas datagrame.

    Parameters:
    ----------
    path : {str}
        The path containing the .xml files
    Returns
    -------
    Pandas DataFrame
        The produced dataframe
    """
    import xml.etree.ElementTree as ET
    import glob

    xml_list = []
    for xml_file in glob.glob(path + '/*.xml'):
        tree = ET.parse(xml_file)
        root = tree.getroot()
        for member in root.findall('object'):
            try:
                value = (root.find('filename').text,
                        int(root.find('size')[0].text),
                        int(root.find('size')[1].text),
                        member[0].text,
                        int(member[4][0].text),
                        int(member[4][1].text),
                        int(member[4][2].text),
                        int(member[4][3].text)
                        )
                xml_list.append(value)
            except Exception as e:
                pass
                
    column_name = ['filename', 'width', 'height',
                'class', 'xmin', 'ymin', 'xmax', 'ymax']
    xml_df = pd.DataFrame(xml_list, columns=column_name)
    le = preprocessing.LabelEncoder()
    xml_df['label'] = le.fit_transform(xml_df['class'])
    return xml_df


# In[4]:


def split(df, group):

    from collections import namedtuple, OrderedDict
    data = namedtuple('data', ['filename', 'object'])
    gb = df.groupby(group)
    return [data(filename, gb.get_group(x)) for filename, x in zip(gb.groups.keys(), gb.groups)]

def split_data(df, train_size, path=''):
    
    
    all_files = df['filename'].unique()
    
    #Split images between training and test
    
    #80% of the data will be used for training
    mask = np.random.rand(all_files.shape[0]) < train_size

    #Get Training and Test images
    train_images = all_files[mask]
    test_images = all_files[~mask] 

    #Split dataframe between training and test
    train_df = df[df['filename'].isin(train_images)]
    test_df = df[df['filename'].isin(test_images)]

    return train_df, test_df


def create_augmenter_fn(augmenters):
    def augmenter_fn(inputs):
        for augmenter in augmenters:
            inputs = augmenter(inputs)
        return inputs

    return augmenter_fn

def apply_augumentations(dataset, augmenters):

    augmenter_fn = create_augmenter_fn(augmenters)
    dataset = dataset.map(augmenter_fn, num_parallel_calls=tf.data.AUTOTUNE)
    return dataset

def visualize_dataset(inputs, rows, cols, class_mapping, bounding_box_format='xywh', value_range=(0,255)):

    from keras_cv import visualization
    inputs = next(iter(inputs.take(1)))
    images, bounding_boxes = inputs["images"], inputs["bounding_boxes"]
    visualization.plot_bounding_box_gallery(
        images,
        value_range=value_range,
        rows=rows,
        cols=cols,
        y_true=bounding_boxes,
        scale=5,
        font_scale=0.7,
        bounding_box_format=bounding_box_format,
        class_mapping=class_mapping,
    )

def unpackage_raw_tfds_inputs(inputs):
    filepath = inputs['image']
    img = tf.io.read_file(filepath)
    img_array = tf.io.decode_jpeg(img, channels=3)
    bounding_boxes = {
        "classes": inputs['classes'],
        "boxes": inputs['boxes'],
    }
    return {'images':img_array, 'bounding_boxes':bounding_boxes}

def get_class_mapping(df):
    return dict(zip(df['label'], df['class']))
    
def dict_to_tuple(inputs):
    import keras_cv
    return inputs["images"], keras_cv.bounding_box.to_dense(
        inputs["bounding_boxes"], max_boxes=32
    )

def prepare_data(ds):

    ds = ds.map(dict_to_tuple, num_parallel_calls=tf.data.AUTOTUNE)
    ds = ds.prefetch(tf.data.AUTOTUNE)
    return ds

def get_dataset(df, img_path, augumenters, batch_size=4):

    path = os.path.join(os.getcwd(), img_path)
    grouped = split(df, 'filename')
    all_images = []
    all_boxes = []
    all_labels = []

    for group in grouped:

        filepath = os.path.join(path,group.filename)
        xywh = []
        labels = []

        for index, row in group.object.iterrows():
            xmin = row['xmin']
            ymin = row['ymin']
            xmax = row['xmax']
            ymax = row['ymax']
            xywh.append([xmin, ymin,xmax-xmin,ymax-ymin])
            labels.append(row['label'])
    
        all_images.append(filepath)
        all_boxes.append(xywh)
        all_labels.append(labels)
    
    ds = tf.data.Dataset.from_tensor_slices({'image':all_images,
                                         'boxes':tf.ragged.constant(all_boxes), 
                                         'classes':tf.ragged.constant(all_labels)})
                                       
    ds = ds.map(lambda x: unpackage_raw_tfds_inputs(x))
    ds = ds.shuffle(batch_size * 4)
    ds = ds.ragged_batch(batch_size, drop_remainder=True)

    if augumenters is not None:
        ds = apply_augumentations(ds, augumenters)
    return ds


#Function to get predictions from a Detection model
def detector_prediction(model, image_file, label_class_dict,img_array=None, confidence_threshold=0.5):

    """
    image_file: File path of the image for which prediction needs to be done
    img_array: only considered if image_file is not specified
    confidence_threshold: Minimum confidence/probability for prediction to be considered
    """
    #Load image
    if(image_file):
        img = tf.keras.preprocessing.image.load_img(image_file)
        img_array = tf.keras.preprocessing.image.img_to_array(img).astype('uint8')
    
    #Make it a batch of one example
    img_array = tf.expand_dims(img_array, axis=0)

    #Prediction
    output = model(img_array) #get list of tensors discussed above as output
    
    #print(output)
    detection_classes = output['detection_classes'].numpy()[0]
    detection_scores = output['detection_scores'].numpy()[0] #get detection scores
    detection_boxes = output['detection_boxes'].numpy()[0]

    #Select predictions for which probability is higher than confidence_threshold
    selected_predictions = detection_scores >= confidence_threshold

    selected_prediction_scores = detection_scores[selected_predictions]
    selected_prediction_classes = detection_classes[selected_predictions]
    selected_prediction_boxes = detection_boxes[selected_predictions]

    #De-normalize box co-ordinates (multiply x-coordinates by image width and y-coords by image height)
    img_h, img_w = img_array.shape[1:3]

    for i in range(selected_prediction_boxes.shape[0]):
        
        selected_prediction_boxes[i,0] *= img_h #ymin * img_w
        selected_prediction_boxes[i,1] *= img_w #xmin * img_h
        selected_prediction_boxes[i,2] *= img_h #ymax * img_w
        selected_prediction_boxes[i,3] *= img_w #xmax * img_h

    #Make all co-ordinates as integer
    selected_prediction_boxes= selected_prediction_boxes.astype(int)

    #Convert class indexes to actual class labels
    predicted_classes = []
    for i in range(selected_prediction_classes.shape[0]):
        predicted_classes.append(label_class_dict[str(int(selected_prediction_classes[i]))])

    #Number of predictions
    selected_num_predictions = selected_prediction_boxes.shape[0]

    return {'Total Predictions': selected_num_predictions,
            'Classes': predicted_classes, 
            'Scores': selected_prediction_scores, 
            'Box coordinates': selected_prediction_boxes}


# In[16]:


def visualize_output(output, image_file, confidence_threshold=0.5):

    #Read image
    img = cv2.imread(image_file)

    #Draw rectangle for predicted boxes, also add predicted classes
    for i in range(output['Box coordinates'].shape[0]):

        box = output['Box coordinates'][i]
        
        #Draw rectangle - (ymin, xmin, ymax, xmax)
        img = cv2.rectangle(img, (box[1], box[0]), (box[3], box[2]), (0,255,0), 2)
        
        #Add Label - Class name and confidence level
        label = output['Classes'][i] + ': ' + str(round(output['Scores'][i],2))
        img = cv2.putText(img, label, (box[1], box[0]-10), cv2.FONT_HERSHEY_SIMPLEX, 1, (255,255,255), 2, cv2.LINE_AA)
    
    #Conver BGR image to RGB to use with Matplotlib
    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

    #Display image
    plt.figure(figsize=(10,6))
    plt.imshow(img)
    plt.show()


# In[17]:


def video_predict(model, label_class_dict, video_name=0, 
        confidence_threshold=0.5, save_path=None):

    """
    video_name: filepath of the video file on which you want to predict. Ignore 
    this parameter if want to test on webcam (will not work on Google colab)

    confidence_threshold: threhold that model should use to decide if a object
    is really there in the image 

    save_path: If you want to save the displayed results, provide the file path 
    for the generated mp4 file.
    """

    #Load video
    capture = cv2.VideoCapture(video_name)
    
    #Get the first frame
    hasFrame, frame = capture.read()

    img_h = frame.shape[0]
    img_w = frame.shape[1]
    
    #if save_path given, initialize video writer
    if save_path:
        _fourcc = cv2.VideoWriter_fourcc(*'VP90')
        _out = cv2.VideoWriter(save_path, _fourcc, 25, (img_w,img_h))
    
    num_frames = 0
    while hasFrame:
        
        try:
            #Convert frame to RGB color as opencv reads it in BGR format
            img = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

            #Make it a batch of one example
            #img_array = tf.expand_dims(img, axis=0)

            #Call model prediction function above
            output = detector_prediction(model, None, label_class_dict, img_array=img, confidence_threshold=confidence_threshold)

            #Draw rectangle for predicted boxes, also add predicted classes
            for i in range(output['Box coordinates'].shape[0]):

                box = output['Box coordinates'][i]
                
                #Draw rectangle 
                frame = cv2.rectangle(frame, (box[1], box[0]), (box[3], box[2]), (0,255,0), 2)
        
                #Add Label - Class name and confidence level
                label = output['Classes'][i] + ': ' + str(round(output['Scores'][i],2))
                frame = cv2.putText(frame, label, (box[1], box[0]-10), cv2.FONT_HERSHEY_SIMPLEX, 1, 
                                    (255,255,255), 2, cv2.LINE_AA)
    
            
            #Save frame to the output file
            if save_path:
                _out.write(frame)
            
            #This will work normally but not in Google colab
            if save_path is None:
                cv2.imshow('Video', frame)
                if cv2.waitKey(1) & 0xFF == ord('q'):
                    cv2.destroyAllWindows()
                    break

            #Read the next frame
            hasFrame, frame = capture.read()
        except Exception as e:
            print(e)
            hasFrame, frame = capture.read()
        num_frames += 1
    if save_path is None:
        cv2.destroyAllWindows()
    capture.release()
    
    print('Number of frames processed', num_frames)
    #Close the output video file
    if save_path:
        _out.release()