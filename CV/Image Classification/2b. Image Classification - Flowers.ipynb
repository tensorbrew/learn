{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"x182yUaaGOIx"},"source":["### Download Flowers dataset"]},{"cell_type":"code","metadata":{"id":"USgEKN9GGf60","executionInfo":{"status":"ok","timestamp":1709441916643,"user_tz":-330,"elapsed":987,"user":{"displayName":"Rajeev Kumar","userId":"10567937244174773728"}}},"source":["#You can download the data manually as well instead of using 'wget'\n","!wget http://download.tensorflow.org/example_images/flower_photos.tgz --quiet"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"3cCPVwdxpDvR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709441930774,"user_tz":-330,"elapsed":423,"user":{"displayName":"Rajeev Kumar","userId":"10567937244174773728"}},"outputId":"b188ac9f-22b3-49ac-925d-46551ca624e3"},"source":["#Check if file is downloaded\n","!ls -l"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["total 223460\n","-rw-r--r-- 1 root root 228813984 Feb 10  2016 flower_photos.tgz\n","drwxr-xr-x 1 root root      4096 Feb 28 14:27 sample_data\n"]}]},{"cell_type":"code","metadata":{"id":"r9NeLYrOpIc0","executionInfo":{"status":"ok","timestamp":1709441934349,"user_tz":-330,"elapsed":2347,"user":{"displayName":"Rajeev Kumar","userId":"10567937244174773728"}}},"source":["#Unzip the data\n","!tar -xf flower_photos.tgz"],"execution_count":3,"outputs":[]},{"cell_type":"code","metadata":{"id":"fd2Zz_mYrU7N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709441935049,"user_tz":-330,"elapsed":701,"user":{"displayName":"Rajeev Kumar","userId":"10567937244174773728"}},"outputId":"a68d9b1d-4b3c-4dc6-feea-578f1997f5c1"},"source":["#Check how data is organized\n","!ls -l flower_photos"],"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["total 608\n","drwx------ 2 270850 5000  36864 Feb 10  2016 daisy\n","drwx------ 2 270850 5000  49152 Feb 10  2016 dandelion\n","-rw-r----- 1 270850 5000 418049 Feb  9  2016 LICENSE.txt\n","drwx------ 2 270850 5000  36864 Feb 10  2016 roses\n","drwx------ 2 270850 5000  36864 Feb 10  2016 sunflowers\n","drwx------ 2 270850 5000  40960 Feb 10  2016 tulips\n"]}]},{"cell_type":"markdown","metadata":{"id":"Lz0v83zDhs2y"},"source":["### Build batch generator"]},{"cell_type":"code","metadata":{"id":"-da8Sz_BpnpI","executionInfo":{"status":"ok","timestamp":1709441953544,"user_tz":-330,"elapsed":3890,"user":{"displayName":"Rajeev Kumar","userId":"10567937244174773728"}}},"source":["import tensorflow as tf"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"dQQ7Oksaq7tI","executionInfo":{"status":"ok","timestamp":1709441961930,"user_tz":-330,"elapsed":450,"user":{"displayName":"Rajeev Kumar","userId":"10567937244174773728"}}},"source":["#Define some parameters\n","img_size = 60\n","img_depth = 3"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FBkyCTo1qWMy"},"source":["Create an ImageDataGenerator object, it can also split data between train and test."]},{"cell_type":"code","metadata":{"id":"bkUsCc6zp7Kp","executionInfo":{"status":"ok","timestamp":1709441987762,"user_tz":-330,"elapsed":4,"user":{"displayName":"Rajeev Kumar","userId":"10567937244174773728"}}},"source":["#ImageDataGenerator declaration with 20% data as test (80% for training)\n","img_generator= tf.keras.preprocessing.image.ImageDataGenerator(validation_split=0.2)"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q3BVmtXdrHyh"},"source":["ImageDataGenerator can read images directory and also resize them if needed"]},{"cell_type":"code","metadata":{"id":"IkCVDrPOqDjE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709442063677,"user_tz":-330,"elapsed":418,"user":{"displayName":"Rajeev Kumar","userId":"10567937244174773728"}},"outputId":"2fcbd6d9-a80c-4323-ebf2-d62aefc55404"},"source":["#Build training generator.\n","train_generator = img_generator.flow_from_directory('flower_photos',\n","                                                    batch_size=64,\n","                                                    target_size=(img_size, img_size),\n","                                                    subset='training')"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 2939 images belonging to 5 classes.\n"]}]},{"cell_type":"code","metadata":{"id":"8Z7_0KoJGRDM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709442111731,"user_tz":-330,"elapsed":399,"user":{"displayName":"Rajeev Kumar","userId":"10567937244174773728"}},"outputId":"9da51472-6a98-4473-bdba-e389fe94727f"},"source":["#Build test generator\n","test_generator = img_generator.flow_from_directory('flower_photos',\n","                                                   target_size=(img_size, img_size),\n","                                                   subset='validation',\n","                                                   batch_size=64)"],"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Found 731 images belonging to 5 classes.\n"]}]},{"cell_type":"code","metadata":{"id":"ZnOY195Pt7mn"},"source":["type(train_generator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dCpXm9s4rjM1"},"source":["ImageDataGenerator returns 64 images and their labels"]},{"cell_type":"code","metadata":{"id":"aUhNi9Krrpq7","executionInfo":{"status":"ok","timestamp":1709442167688,"user_tz":-330,"elapsed":421,"user":{"displayName":"Rajeev Kumar","userId":"10567937244174773728"}}},"source":["#Lets check the features (images) and Labels (flower class) returned by ImageDataGenerator\n","X, y = next(train_generator)"],"execution_count":11,"outputs":[]},{"cell_type":"code","source":["print('Input features shape', X.shape)\n","print('Actual labels shape', y.shape)"],"metadata":{"id":"g_QIPMTvsZZV","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709442176765,"user_tz":-330,"elapsed":398,"user":{"displayName":"Rajeev Kumar","userId":"10567937244174773728"}},"outputId":"fd1daca4-548d-404c-fb24-b0334ae0c529"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Input features shape (64, 60, 60, 3)\n","Actual labels shape (64, 5)\n"]}]},{"cell_type":"code","metadata":{"id":"1NUb1CcQx_Zo","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709442207499,"user_tz":-330,"elapsed":388,"user":{"displayName":"Rajeev Kumar","userId":"10567937244174773728"}},"outputId":"f5b72010-da38-42f8-ddad-948f28144254"},"source":["y[0]"],"execution_count":13,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0., 0., 0., 1., 0.], dtype=float32)"]},"metadata":{},"execution_count":13}]},{"cell_type":"code","metadata":{"id":"_iJgvxvh32gt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679740134807,"user_tz":-330,"elapsed":3,"user":{"displayName":"Rajeev Kumar","userId":"10567937244174773728"}},"outputId":"2d5acfee-c89c-4e0f-bb35-5792e99cd600"},"source":["import numpy as np\n","np.unique(X[0])"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n","        11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n","        22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,\n","        33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,\n","        44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,\n","        55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,\n","        66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,\n","        77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,\n","        88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,\n","        99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,\n","       110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,\n","       121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,\n","       132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,\n","       143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,\n","       154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,\n","       165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.,\n","       176., 177., 178., 179., 180., 181., 182., 183., 184., 185., 186.,\n","       187., 188., 189., 190., 191., 192., 193., 194., 195., 196., 197.,\n","       198., 199., 200., 201., 202., 203., 204., 205., 206., 207., 208.,\n","       209., 210., 211., 212., 213., 214., 215., 216., 217., 218., 219.,\n","       220., 221., 222., 223., 224., 225., 226., 227., 228., 229., 230.,\n","       231., 232., 233., 234., 235., 236., 237., 238., 239., 240., 241.,\n","       242., 243., 244., 245., 246., 247., 248., 249., 250., 251., 252.,\n","       253., 254., 255.], dtype=float32)"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","metadata":{"id":"es0UPokXxIKM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709442242879,"user_tz":-330,"elapsed":426,"user":{"displayName":"Rajeev Kumar","userId":"10567937244174773728"}},"outputId":"8d7465de-ee8d-4dce-dbf2-aaae0b1c1a24"},"source":["train_generator.class_indices"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'daisy': 0, 'dandelion': 1, 'roses': 2, 'sunflowers': 3, 'tulips': 4}"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"xDV8kVGpOi_w"},"source":["X[0]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p1DOwu8Bhs29"},"source":["### Build CNN Model"]},{"cell_type":"code","metadata":{"id":"tWQJ4SzZhs2-","executionInfo":{"status":"ok","timestamp":1709443044193,"user_tz":-330,"elapsed":2,"user":{"displayName":"Rajeev Kumar","userId":"10567937244174773728"}}},"source":["#Clear any previous model from memory\n","tf.keras.backend.clear_session()\n","\n","#Initialize model\n","model = tf.keras.models.Sequential()\n","\n","#normalize data\n","model.add(tf.keras.layers.BatchNormalization(input_shape=(img_size,img_size,3,)))\n","\n","#Add Conv Layer\n","model.add(tf.keras.layers.Conv2D(32,\n","                                 kernel_size=(3,3),\n","                                 activation='relu'))"],"execution_count":21,"outputs":[]},{"cell_type":"code","source":["model.output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QVbP_q5hxxme","executionInfo":{"status":"ok","timestamp":1709443044194,"user_tz":-330,"elapsed":2,"user":{"displayName":"Rajeev Kumar","userId":"10567937244174773728"}},"outputId":"f52a3a2d-1b01-49f9-d566-9f4336375277"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<KerasTensor: shape=(None, 58, 58, 32) dtype=float32 (created by layer 'conv2d')>"]},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["#normalize data\n","model.add(tf.keras.layers.BatchNormalization())\n","\n","#Add Conv Layer\n","model.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\n","\n","#normalize data\n","model.add(tf.keras.layers.BatchNormalization())\n","\n","#Add Max Pool layer\n","model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n","\n","#Add Dense Layers after flattening the data\n","model.add(tf.keras.layers.Flatten())\n","model.add(tf.keras.layers.Dense(128, activation='relu'))\n","\n","#Add Dropout\n","model.add(tf.keras.layers.Dropout(0.25))\n","\n","#Add Output Layer\n","model.add(tf.keras.layers.Dense(5, activation='softmax'))"],"metadata":{"id":"8WhjR0Tsxr9Q","executionInfo":{"status":"ok","timestamp":1709443044820,"user_tz":-330,"elapsed":628,"user":{"displayName":"Rajeev Kumar","userId":"10567937244174773728"}}},"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"GsuEXbofhs3D","executionInfo":{"status":"ok","timestamp":1709443045727,"user_tz":-330,"elapsed":2,"user":{"displayName":"Rajeev Kumar","userId":"10567937244174773728"}}},"source":["#Specify Loass and Optimizer\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"],"execution_count":24,"outputs":[]},{"cell_type":"code","source":["#Model Summary\n","model.summary()"],"metadata":{"id":"KCB11Qm44ynY","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1709443052146,"user_tz":-330,"elapsed":8,"user":{"displayName":"Rajeev Kumar","userId":"10567937244174773728"}},"outputId":"bee9f496-ff21-4aeb-8ced-275ff528b1ab"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," batch_normalization (Batch  (None, 60, 60, 3)         12        \n"," Normalization)                                                  \n","                                                                 \n"," conv2d (Conv2D)             (None, 58, 58, 32)        896       \n","                                                                 \n"," batch_normalization_1 (Bat  (None, 58, 58, 32)        128       \n"," chNormalization)                                                \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 56, 56, 64)        18496     \n","                                                                 \n"," batch_normalization_2 (Bat  (None, 56, 56, 64)        256       \n"," chNormalization)                                                \n","                                                                 \n"," max_pooling2d (MaxPooling2  (None, 28, 28, 64)        0         \n"," D)                                                              \n","                                                                 \n"," flatten (Flatten)           (None, 50176)             0         \n","                                                                 \n"," dense (Dense)               (None, 128)               6422656   \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 645       \n","                                                                 \n","=================================================================\n","Total params: 6443089 (24.58 MB)\n","Trainable params: 6442891 (24.58 MB)\n","Non-trainable params: 198 (792.00 Byte)\n","_________________________________________________________________\n"]}]},{"cell_type":"markdown","metadata":{"id":"0v7UlgC5hs3g"},"source":["### Train the model"]},{"cell_type":"code","metadata":{"id":"cklB73X8yCvt","executionInfo":{"status":"ok","timestamp":1709442389061,"user_tz":-330,"elapsed":407,"user":{"displayName":"Rajeev Kumar","userId":"10567937244174773728"}}},"source":["model_checkpoint = tf.keras.callbacks.ModelCheckpoint('flowers.h5',\n","                                                      save_best_only=True,\n","                                                      monitor='val_accuracy',\n","                                                      mode='max',\n","                                                      verbose=1)"],"execution_count":20,"outputs":[]},{"cell_type":"code","metadata":{"id":"ldJt7GHB5dtQ"},"source":["2939//64"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7YGqTxKQhs3l","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668856266064,"user_tz":-330,"elapsed":753495,"user":{"displayName":"Rajeev Kumar","userId":"10567937244174773728"}},"outputId":"09cb64f5-e903-49f7-d14c-5c090062c73d"},"source":["model.fit(train_generator,\n","          epochs=200,\n","          steps_per_epoch= 2939//64,  #Number of batches per epoch\n","          validation_data=test_generator,\n","          validation_steps = 731//64,\n","          callbacks=[model_checkpoint]) #Number of test images//batch_size"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/200\n","45/45 [==============================] - ETA: 0s - loss: 4.0857 - accuracy: 0.3621\n","Epoch 1: val_accuracy improved from -inf to 0.16761, saving model to flowers.h5\n","45/45 [==============================] - 18s 228ms/step - loss: 4.0857 - accuracy: 0.3621 - val_loss: 1.8344 - val_accuracy: 0.1676\n","Epoch 2/200\n","45/45 [==============================] - ETA: 0s - loss: 1.3971 - accuracy: 0.4539\n","Epoch 2: val_accuracy improved from 0.16761 to 0.29403, saving model to flowers.h5\n","45/45 [==============================] - 10s 220ms/step - loss: 1.3971 - accuracy: 0.4539 - val_loss: 5.1972 - val_accuracy: 0.2940\n","Epoch 3/200\n","45/45 [==============================] - ETA: 0s - loss: 1.2128 - accuracy: 0.4859\n","Epoch 3: val_accuracy improved from 0.29403 to 0.34659, saving model to flowers.h5\n","45/45 [==============================] - 9s 207ms/step - loss: 1.2128 - accuracy: 0.4859 - val_loss: 12.7614 - val_accuracy: 0.3466\n","Epoch 4/200\n","45/45 [==============================] - ETA: 0s - loss: 1.1279 - accuracy: 0.5363\n","Epoch 4: val_accuracy did not improve from 0.34659\n","45/45 [==============================] - 9s 200ms/step - loss: 1.1279 - accuracy: 0.5363 - val_loss: 17.1957 - val_accuracy: 0.3239\n","Epoch 5/200\n","45/45 [==============================] - ETA: 0s - loss: 1.0650 - accuracy: 0.5551\n","Epoch 5: val_accuracy improved from 0.34659 to 0.39773, saving model to flowers.h5\n","45/45 [==============================] - 10s 222ms/step - loss: 1.0650 - accuracy: 0.5551 - val_loss: 10.5754 - val_accuracy: 0.3977\n","Epoch 6/200\n","45/45 [==============================] - ETA: 0s - loss: 0.9134 - accuracy: 0.5969\n","Epoch 6: val_accuracy improved from 0.39773 to 0.47159, saving model to flowers.h5\n","45/45 [==============================] - 10s 226ms/step - loss: 0.9134 - accuracy: 0.5969 - val_loss: 6.9278 - val_accuracy: 0.4716\n","Epoch 7/200\n","45/45 [==============================] - ETA: 0s - loss: 0.8188 - accuracy: 0.6435\n","Epoch 7: val_accuracy improved from 0.47159 to 0.53693, saving model to flowers.h5\n","45/45 [==============================] - 9s 205ms/step - loss: 0.8188 - accuracy: 0.6435 - val_loss: 4.9408 - val_accuracy: 0.5369\n","Epoch 8/200\n","45/45 [==============================] - ETA: 0s - loss: 0.7536 - accuracy: 0.6630\n","Epoch 8: val_accuracy improved from 0.53693 to 0.57386, saving model to flowers.h5\n","45/45 [==============================] - 9s 204ms/step - loss: 0.7536 - accuracy: 0.6630 - val_loss: 2.7768 - val_accuracy: 0.5739\n","Epoch 9/200\n","45/45 [==============================] - ETA: 0s - loss: 0.6827 - accuracy: 0.6883\n","Epoch 9: val_accuracy did not improve from 0.57386\n","45/45 [==============================] - 10s 217ms/step - loss: 0.6827 - accuracy: 0.6883 - val_loss: 2.3149 - val_accuracy: 0.5568\n","Epoch 10/200\n","45/45 [==============================] - ETA: 0s - loss: 0.6155 - accuracy: 0.7214\n","Epoch 10: val_accuracy improved from 0.57386 to 0.59375, saving model to flowers.h5\n","45/45 [==============================] - 10s 224ms/step - loss: 0.6155 - accuracy: 0.7214 - val_loss: 1.9528 - val_accuracy: 0.5938\n","Epoch 11/200\n","45/45 [==============================] - ETA: 0s - loss: 0.5911 - accuracy: 0.7329\n","Epoch 11: val_accuracy did not improve from 0.59375\n","45/45 [==============================] - 9s 199ms/step - loss: 0.5911 - accuracy: 0.7329 - val_loss: 1.7037 - val_accuracy: 0.5653\n","Epoch 12/200\n","45/45 [==============================] - ETA: 0s - loss: 0.6205 - accuracy: 0.7217\n","Epoch 12: val_accuracy did not improve from 0.59375\n","45/45 [==============================] - 9s 200ms/step - loss: 0.6205 - accuracy: 0.7217 - val_loss: 1.3841 - val_accuracy: 0.5895\n","Epoch 13/200\n","45/45 [==============================] - ETA: 0s - loss: 0.5290 - accuracy: 0.7537\n","Epoch 13: val_accuracy did not improve from 0.59375\n","45/45 [==============================] - 9s 200ms/step - loss: 0.5290 - accuracy: 0.7537 - val_loss: 1.6198 - val_accuracy: 0.5597\n","Epoch 14/200\n","45/45 [==============================] - ETA: 0s - loss: 0.4905 - accuracy: 0.7837\n","Epoch 14: val_accuracy improved from 0.59375 to 0.59801, saving model to flowers.h5\n","45/45 [==============================] - 9s 200ms/step - loss: 0.4905 - accuracy: 0.7837 - val_loss: 1.5474 - val_accuracy: 0.5980\n","Epoch 15/200\n","45/45 [==============================] - ETA: 0s - loss: 0.5477 - accuracy: 0.7774\n","Epoch 15: val_accuracy did not improve from 0.59801\n","45/45 [==============================] - 10s 217ms/step - loss: 0.5477 - accuracy: 0.7774 - val_loss: 1.7623 - val_accuracy: 0.5469\n","Epoch 16/200\n","45/45 [==============================] - ETA: 0s - loss: 0.5445 - accuracy: 0.7677\n","Epoch 16: val_accuracy did not improve from 0.59801\n","45/45 [==============================] - 9s 197ms/step - loss: 0.5445 - accuracy: 0.7677 - val_loss: 1.6645 - val_accuracy: 0.5355\n","Epoch 17/200\n","45/45 [==============================] - ETA: 0s - loss: 0.4602 - accuracy: 0.7910\n","Epoch 17: val_accuracy did not improve from 0.59801\n","45/45 [==============================] - 9s 196ms/step - loss: 0.4602 - accuracy: 0.7910 - val_loss: 1.8717 - val_accuracy: 0.5909\n","Epoch 18/200\n","45/45 [==============================] - ETA: 0s - loss: 0.4312 - accuracy: 0.8059\n","Epoch 18: val_accuracy did not improve from 0.59801\n","45/45 [==============================] - 9s 199ms/step - loss: 0.4312 - accuracy: 0.8059 - val_loss: 1.6355 - val_accuracy: 0.5881\n","Epoch 19/200\n","45/45 [==============================] - ETA: 0s - loss: 0.4529 - accuracy: 0.7944\n","Epoch 19: val_accuracy did not improve from 0.59801\n","45/45 [==============================] - 10s 222ms/step - loss: 0.4529 - accuracy: 0.7944 - val_loss: 1.7101 - val_accuracy: 0.5625\n","Epoch 20/200\n","45/45 [==============================] - ETA: 0s - loss: 0.4005 - accuracy: 0.8247\n","Epoch 20: val_accuracy did not improve from 0.59801\n","45/45 [==============================] - 10s 210ms/step - loss: 0.4005 - accuracy: 0.8247 - val_loss: 1.4932 - val_accuracy: 0.5866\n","Epoch 21/200\n","45/45 [==============================] - ETA: 0s - loss: 0.3875 - accuracy: 0.8209\n","Epoch 21: val_accuracy improved from 0.59801 to 0.61080, saving model to flowers.h5\n","45/45 [==============================] - 9s 205ms/step - loss: 0.3875 - accuracy: 0.8209 - val_loss: 1.5301 - val_accuracy: 0.6108\n","Epoch 22/200\n","45/45 [==============================] - ETA: 0s - loss: 0.3562 - accuracy: 0.8341\n","Epoch 22: val_accuracy did not improve from 0.61080\n","45/45 [==============================] - 9s 200ms/step - loss: 0.3562 - accuracy: 0.8341 - val_loss: 1.6335 - val_accuracy: 0.5767\n","Epoch 23/200\n","45/45 [==============================] - ETA: 0s - loss: 0.3407 - accuracy: 0.8560\n","Epoch 23: val_accuracy did not improve from 0.61080\n","45/45 [==============================] - 9s 203ms/step - loss: 0.3407 - accuracy: 0.8560 - val_loss: 1.7573 - val_accuracy: 0.5724\n","Epoch 24/200\n","45/45 [==============================] - ETA: 0s - loss: 0.3686 - accuracy: 0.8306\n","Epoch 24: val_accuracy did not improve from 0.61080\n","45/45 [==============================] - 10s 218ms/step - loss: 0.3686 - accuracy: 0.8306 - val_loss: 1.8456 - val_accuracy: 0.6023\n","Epoch 25/200\n","45/45 [==============================] - ETA: 0s - loss: 0.3795 - accuracy: 0.8407\n","Epoch 25: val_accuracy did not improve from 0.61080\n","45/45 [==============================] - 9s 201ms/step - loss: 0.3795 - accuracy: 0.8407 - val_loss: 1.8885 - val_accuracy: 0.5653\n","Epoch 26/200\n","45/45 [==============================] - ETA: 0s - loss: 0.3869 - accuracy: 0.8379\n","Epoch 26: val_accuracy improved from 0.61080 to 0.62074, saving model to flowers.h5\n","45/45 [==============================] - 9s 211ms/step - loss: 0.3869 - accuracy: 0.8379 - val_loss: 1.5911 - val_accuracy: 0.6207\n","Epoch 27/200\n","45/45 [==============================] - ETA: 0s - loss: 0.3413 - accuracy: 0.8417\n","Epoch 27: val_accuracy did not improve from 0.62074\n","45/45 [==============================] - 9s 198ms/step - loss: 0.3413 - accuracy: 0.8417 - val_loss: 1.6835 - val_accuracy: 0.6009\n","Epoch 28/200\n","45/45 [==============================] - ETA: 0s - loss: 0.3136 - accuracy: 0.8602\n","Epoch 28: val_accuracy did not improve from 0.62074\n","45/45 [==============================] - 10s 219ms/step - loss: 0.3136 - accuracy: 0.8602 - val_loss: 1.9540 - val_accuracy: 0.5881\n","Epoch 29/200\n","45/45 [==============================] - ETA: 0s - loss: 0.3033 - accuracy: 0.8689\n","Epoch 29: val_accuracy did not improve from 0.62074\n","45/45 [==============================] - 9s 198ms/step - loss: 0.3033 - accuracy: 0.8689 - val_loss: 2.1191 - val_accuracy: 0.5852\n","Epoch 30/200\n","45/45 [==============================] - ETA: 0s - loss: 0.2879 - accuracy: 0.8720\n","Epoch 30: val_accuracy did not improve from 0.62074\n","45/45 [==============================] - 9s 200ms/step - loss: 0.2879 - accuracy: 0.8720 - val_loss: 1.6638 - val_accuracy: 0.6151\n","Epoch 31/200\n","45/45 [==============================] - ETA: 0s - loss: 0.2872 - accuracy: 0.8734\n","Epoch 31: val_accuracy did not improve from 0.62074\n","45/45 [==============================] - 9s 199ms/step - loss: 0.2872 - accuracy: 0.8734 - val_loss: 2.1067 - val_accuracy: 0.6037\n","Epoch 32/200\n","45/45 [==============================] - ETA: 0s - loss: 0.2506 - accuracy: 0.8963\n","Epoch 32: val_accuracy did not improve from 0.62074\n","45/45 [==============================] - 9s 200ms/step - loss: 0.2506 - accuracy: 0.8963 - val_loss: 1.8346 - val_accuracy: 0.6080\n","Epoch 33/200\n","45/45 [==============================] - ETA: 0s - loss: 0.2554 - accuracy: 0.8894\n","Epoch 33: val_accuracy did not improve from 0.62074\n","45/45 [==============================] - 10s 217ms/step - loss: 0.2554 - accuracy: 0.8894 - val_loss: 1.7591 - val_accuracy: 0.6122\n","Epoch 34/200\n","45/45 [==============================] - ETA: 0s - loss: 0.2363 - accuracy: 0.8925\n","Epoch 34: val_accuracy improved from 0.62074 to 0.62926, saving model to flowers.h5\n","45/45 [==============================] - 9s 202ms/step - loss: 0.2363 - accuracy: 0.8925 - val_loss: 2.1556 - val_accuracy: 0.6293\n","Epoch 35/200\n","45/45 [==============================] - ETA: 0s - loss: 0.2149 - accuracy: 0.9040\n","Epoch 35: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 9s 197ms/step - loss: 0.2149 - accuracy: 0.9040 - val_loss: 1.8274 - val_accuracy: 0.6236\n","Epoch 36/200\n","45/45 [==============================] - ETA: 0s - loss: 0.2291 - accuracy: 0.9012\n","Epoch 36: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 9s 198ms/step - loss: 0.2291 - accuracy: 0.9012 - val_loss: 2.0549 - val_accuracy: 0.6165\n","Epoch 37/200\n","45/45 [==============================] - ETA: 0s - loss: 0.2270 - accuracy: 0.9016\n","Epoch 37: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 10s 227ms/step - loss: 0.2270 - accuracy: 0.9016 - val_loss: 2.3802 - val_accuracy: 0.5852\n","Epoch 38/200\n","45/45 [==============================] - ETA: 0s - loss: 0.2671 - accuracy: 0.8817\n","Epoch 38: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 9s 199ms/step - loss: 0.2671 - accuracy: 0.8817 - val_loss: 1.9459 - val_accuracy: 0.6065\n","Epoch 39/200\n","45/45 [==============================] - ETA: 0s - loss: 0.2523 - accuracy: 0.8939\n","Epoch 39: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 9s 200ms/step - loss: 0.2523 - accuracy: 0.8939 - val_loss: 2.3891 - val_accuracy: 0.5724\n","Epoch 40/200\n","45/45 [==============================] - ETA: 0s - loss: 0.2341 - accuracy: 0.8991\n","Epoch 40: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 10s 216ms/step - loss: 0.2341 - accuracy: 0.8991 - val_loss: 2.2050 - val_accuracy: 0.5909\n","Epoch 41/200\n","45/45 [==============================] - ETA: 0s - loss: 0.2409 - accuracy: 0.8929\n","Epoch 41: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 10s 217ms/step - loss: 0.2409 - accuracy: 0.8929 - val_loss: 2.2401 - val_accuracy: 0.6165\n","Epoch 42/200\n","45/45 [==============================] - ETA: 0s - loss: 0.2280 - accuracy: 0.8991\n","Epoch 42: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 9s 196ms/step - loss: 0.2280 - accuracy: 0.8991 - val_loss: 2.1381 - val_accuracy: 0.6136\n","Epoch 43/200\n","45/45 [==============================] - ETA: 0s - loss: 0.2252 - accuracy: 0.9016\n","Epoch 43: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 9s 196ms/step - loss: 0.2252 - accuracy: 0.9016 - val_loss: 2.1570 - val_accuracy: 0.6122\n","Epoch 44/200\n","45/45 [==============================] - ETA: 0s - loss: 0.2126 - accuracy: 0.8991\n","Epoch 44: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 9s 198ms/step - loss: 0.2126 - accuracy: 0.8991 - val_loss: 2.2295 - val_accuracy: 0.6080\n","Epoch 45/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1918 - accuracy: 0.9078\n","Epoch 45: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 10s 217ms/step - loss: 0.1918 - accuracy: 0.9078 - val_loss: 2.1299 - val_accuracy: 0.5966\n","Epoch 46/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1805 - accuracy: 0.9176\n","Epoch 46: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 9s 197ms/step - loss: 0.1805 - accuracy: 0.9176 - val_loss: 2.8070 - val_accuracy: 0.6080\n","Epoch 47/200\n","45/45 [==============================] - ETA: 0s - loss: 0.3129 - accuracy: 0.8821\n","Epoch 47: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 9s 199ms/step - loss: 0.3129 - accuracy: 0.8821 - val_loss: 3.4054 - val_accuracy: 0.5384\n","Epoch 48/200\n","45/45 [==============================] - ETA: 0s - loss: 0.2797 - accuracy: 0.8849\n","Epoch 48: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 10s 217ms/step - loss: 0.2797 - accuracy: 0.8849 - val_loss: 2.2512 - val_accuracy: 0.5980\n","Epoch 49/200\n","45/45 [==============================] - ETA: 0s - loss: 0.2788 - accuracy: 0.8863\n","Epoch 49: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 10s 221ms/step - loss: 0.2788 - accuracy: 0.8863 - val_loss: 2.5471 - val_accuracy: 0.6051\n","Epoch 50/200\n","45/45 [==============================] - ETA: 0s - loss: 0.2492 - accuracy: 0.9023\n","Epoch 50: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 9s 199ms/step - loss: 0.2492 - accuracy: 0.9023 - val_loss: 2.2126 - val_accuracy: 0.5994\n","Epoch 51/200\n","45/45 [==============================] - ETA: 0s - loss: 0.2064 - accuracy: 0.9137\n","Epoch 51: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 9s 196ms/step - loss: 0.2064 - accuracy: 0.9137 - val_loss: 2.7015 - val_accuracy: 0.5710\n","Epoch 52/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1954 - accuracy: 0.9137\n","Epoch 52: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 9s 197ms/step - loss: 0.1954 - accuracy: 0.9137 - val_loss: 2.0496 - val_accuracy: 0.6051\n","Epoch 53/200\n","45/45 [==============================] - ETA: 0s - loss: 0.2110 - accuracy: 0.9057\n","Epoch 53: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 10s 234ms/step - loss: 0.2110 - accuracy: 0.9057 - val_loss: 2.6426 - val_accuracy: 0.5724\n","Epoch 54/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1984 - accuracy: 0.9151\n","Epoch 54: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 9s 201ms/step - loss: 0.1984 - accuracy: 0.9151 - val_loss: 2.2556 - val_accuracy: 0.5966\n","Epoch 55/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1963 - accuracy: 0.9162\n","Epoch 55: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 9s 198ms/step - loss: 0.1963 - accuracy: 0.9162 - val_loss: 2.4989 - val_accuracy: 0.6023\n","Epoch 56/200\n","45/45 [==============================] - ETA: 0s - loss: 0.2018 - accuracy: 0.9137\n","Epoch 56: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 9s 196ms/step - loss: 0.2018 - accuracy: 0.9137 - val_loss: 2.9561 - val_accuracy: 0.6080\n","Epoch 57/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1852 - accuracy: 0.9252\n","Epoch 57: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 10s 220ms/step - loss: 0.1852 - accuracy: 0.9252 - val_loss: 2.3883 - val_accuracy: 0.6108\n","Epoch 58/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1720 - accuracy: 0.9228\n","Epoch 58: val_accuracy did not improve from 0.62926\n","45/45 [==============================] - 9s 198ms/step - loss: 0.1720 - accuracy: 0.9228 - val_loss: 2.6379 - val_accuracy: 0.5980\n","Epoch 59/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1769 - accuracy: 0.9186\n","Epoch 59: val_accuracy improved from 0.62926 to 0.63068, saving model to flowers.h5\n","45/45 [==============================] - 9s 202ms/step - loss: 0.1769 - accuracy: 0.9186 - val_loss: 2.3613 - val_accuracy: 0.6307\n","Epoch 60/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1656 - accuracy: 0.9266\n","Epoch 60: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 9s 199ms/step - loss: 0.1656 - accuracy: 0.9266 - val_loss: 2.4171 - val_accuracy: 0.6207\n","Epoch 61/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1698 - accuracy: 0.9238\n","Epoch 61: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 9s 197ms/step - loss: 0.1698 - accuracy: 0.9238 - val_loss: 2.5082 - val_accuracy: 0.6222\n","Epoch 62/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1814 - accuracy: 0.9290\n","Epoch 62: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 10s 217ms/step - loss: 0.1814 - accuracy: 0.9290 - val_loss: 2.1404 - val_accuracy: 0.6207\n","Epoch 63/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1884 - accuracy: 0.9117\n","Epoch 63: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 9s 198ms/step - loss: 0.1884 - accuracy: 0.9117 - val_loss: 2.6814 - val_accuracy: 0.6122\n","Epoch 64/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1520 - accuracy: 0.9273\n","Epoch 64: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 9s 197ms/step - loss: 0.1520 - accuracy: 0.9273 - val_loss: 2.5818 - val_accuracy: 0.6151\n","Epoch 65/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1586 - accuracy: 0.9297\n","Epoch 65: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 9s 196ms/step - loss: 0.1586 - accuracy: 0.9297 - val_loss: 2.1624 - val_accuracy: 0.6222\n","Epoch 66/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1575 - accuracy: 0.9277\n","Epoch 66: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 10s 215ms/step - loss: 0.1575 - accuracy: 0.9277 - val_loss: 2.6817 - val_accuracy: 0.6065\n","Epoch 67/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1637 - accuracy: 0.9304\n","Epoch 67: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 9s 199ms/step - loss: 0.1637 - accuracy: 0.9304 - val_loss: 2.3086 - val_accuracy: 0.6065\n","Epoch 68/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1754 - accuracy: 0.9228\n","Epoch 68: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 9s 198ms/step - loss: 0.1754 - accuracy: 0.9228 - val_loss: 3.1983 - val_accuracy: 0.5881\n","Epoch 69/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1699 - accuracy: 0.9283\n","Epoch 69: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 9s 198ms/step - loss: 0.1699 - accuracy: 0.9283 - val_loss: 2.6628 - val_accuracy: 0.6023\n","Epoch 70/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1623 - accuracy: 0.9252\n","Epoch 70: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 10s 215ms/step - loss: 0.1623 - accuracy: 0.9252 - val_loss: 2.5522 - val_accuracy: 0.6122\n","Epoch 71/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1674 - accuracy: 0.9263\n","Epoch 71: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 10s 220ms/step - loss: 0.1674 - accuracy: 0.9263 - val_loss: 2.7012 - val_accuracy: 0.6122\n","Epoch 72/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1535 - accuracy: 0.9363\n","Epoch 72: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 9s 199ms/step - loss: 0.1535 - accuracy: 0.9363 - val_loss: 2.5847 - val_accuracy: 0.6165\n","Epoch 73/200\n","45/45 [==============================] - ETA: 0s - loss: 0.2107 - accuracy: 0.9231\n","Epoch 73: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 9s 207ms/step - loss: 0.2107 - accuracy: 0.9231 - val_loss: 2.1089 - val_accuracy: 0.6094\n","Epoch 74/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1655 - accuracy: 0.9350\n","Epoch 74: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 10s 220ms/step - loss: 0.1655 - accuracy: 0.9350 - val_loss: 2.4109 - val_accuracy: 0.6023\n","Epoch 75/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1686 - accuracy: 0.9283\n","Epoch 75: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 10s 218ms/step - loss: 0.1686 - accuracy: 0.9283 - val_loss: 2.8724 - val_accuracy: 0.5824\n","Epoch 76/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1616 - accuracy: 0.9363\n","Epoch 76: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 9s 198ms/step - loss: 0.1616 - accuracy: 0.9363 - val_loss: 2.7557 - val_accuracy: 0.6108\n","Epoch 77/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1605 - accuracy: 0.9308\n","Epoch 77: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 9s 198ms/step - loss: 0.1605 - accuracy: 0.9308 - val_loss: 3.2058 - val_accuracy: 0.5938\n","Epoch 78/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1890 - accuracy: 0.9297\n","Epoch 78: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 9s 199ms/step - loss: 0.1890 - accuracy: 0.9297 - val_loss: 2.5209 - val_accuracy: 0.5682\n","Epoch 79/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1520 - accuracy: 0.9419\n","Epoch 79: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 9s 200ms/step - loss: 0.1520 - accuracy: 0.9419 - val_loss: 2.3763 - val_accuracy: 0.6051\n","Epoch 80/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1462 - accuracy: 0.9384\n","Epoch 80: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 10s 221ms/step - loss: 0.1462 - accuracy: 0.9384 - val_loss: 2.3575 - val_accuracy: 0.6207\n","Epoch 81/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1398 - accuracy: 0.9419\n","Epoch 81: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 9s 199ms/step - loss: 0.1398 - accuracy: 0.9419 - val_loss: 2.4524 - val_accuracy: 0.6250\n","Epoch 82/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1412 - accuracy: 0.9412\n","Epoch 82: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 9s 196ms/step - loss: 0.1412 - accuracy: 0.9412 - val_loss: 2.3808 - val_accuracy: 0.5895\n","Epoch 83/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1479 - accuracy: 0.9443\n","Epoch 83: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 9s 198ms/step - loss: 0.1479 - accuracy: 0.9443 - val_loss: 2.2131 - val_accuracy: 0.6108\n","Epoch 84/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9430\n","Epoch 84: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 9s 197ms/step - loss: 0.1406 - accuracy: 0.9430 - val_loss: 2.4786 - val_accuracy: 0.6207\n","Epoch 85/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1476 - accuracy: 0.9370\n","Epoch 85: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 9s 207ms/step - loss: 0.1476 - accuracy: 0.9370 - val_loss: 2.4434 - val_accuracy: 0.5966\n","Epoch 86/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1379 - accuracy: 0.9485\n","Epoch 86: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 9s 202ms/step - loss: 0.1379 - accuracy: 0.9485 - val_loss: 2.9267 - val_accuracy: 0.6250\n","Epoch 87/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1462 - accuracy: 0.9475\n","Epoch 87: val_accuracy did not improve from 0.63068\n","45/45 [==============================] - 9s 199ms/step - loss: 0.1462 - accuracy: 0.9475 - val_loss: 3.0883 - val_accuracy: 0.6236\n","Epoch 88/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1361 - accuracy: 0.9447\n","Epoch 88: val_accuracy improved from 0.63068 to 0.63210, saving model to flowers.h5\n","45/45 [==============================] - 10s 223ms/step - loss: 0.1361 - accuracy: 0.9447 - val_loss: 2.5183 - val_accuracy: 0.6321\n","Epoch 89/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1292 - accuracy: 0.9506\n","Epoch 89: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 10s 219ms/step - loss: 0.1292 - accuracy: 0.9506 - val_loss: 2.5477 - val_accuracy: 0.6264\n","Epoch 90/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1200 - accuracy: 0.9548\n","Epoch 90: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 9s 199ms/step - loss: 0.1200 - accuracy: 0.9548 - val_loss: 2.9861 - val_accuracy: 0.5952\n","Epoch 91/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1371 - accuracy: 0.9478\n","Epoch 91: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 9s 202ms/step - loss: 0.1371 - accuracy: 0.9478 - val_loss: 2.9996 - val_accuracy: 0.6108\n","Epoch 92/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1219 - accuracy: 0.9517\n","Epoch 92: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 9s 202ms/step - loss: 0.1219 - accuracy: 0.9517 - val_loss: 2.7150 - val_accuracy: 0.6293\n","Epoch 93/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1268 - accuracy: 0.9492\n","Epoch 93: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 10s 219ms/step - loss: 0.1268 - accuracy: 0.9492 - val_loss: 3.1046 - val_accuracy: 0.6065\n","Epoch 94/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1277 - accuracy: 0.9530\n","Epoch 94: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 9s 201ms/step - loss: 0.1277 - accuracy: 0.9530 - val_loss: 3.5920 - val_accuracy: 0.6094\n","Epoch 95/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1192 - accuracy: 0.9583\n","Epoch 95: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 9s 202ms/step - loss: 0.1192 - accuracy: 0.9583 - val_loss: 3.3046 - val_accuracy: 0.6037\n","Epoch 96/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1352 - accuracy: 0.9506\n","Epoch 96: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 9s 202ms/step - loss: 0.1352 - accuracy: 0.9506 - val_loss: 2.2481 - val_accuracy: 0.6207\n","Epoch 97/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1299 - accuracy: 0.9530\n","Epoch 97: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 9s 205ms/step - loss: 0.1299 - accuracy: 0.9530 - val_loss: 2.9655 - val_accuracy: 0.6009\n","Epoch 98/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1521 - accuracy: 0.9496\n","Epoch 98: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 9s 201ms/step - loss: 0.1521 - accuracy: 0.9496 - val_loss: 2.8476 - val_accuracy: 0.5980\n","Epoch 99/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1350 - accuracy: 0.9544\n","Epoch 99: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 9s 200ms/step - loss: 0.1350 - accuracy: 0.9544 - val_loss: 2.7240 - val_accuracy: 0.5938\n","Epoch 100/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1810 - accuracy: 0.9496\n","Epoch 100: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 9s 199ms/step - loss: 0.1810 - accuracy: 0.9496 - val_loss: 3.6434 - val_accuracy: 0.6165\n","Epoch 101/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1759 - accuracy: 0.9405\n","Epoch 101: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 9s 199ms/step - loss: 0.1759 - accuracy: 0.9405 - val_loss: 3.6339 - val_accuracy: 0.5511\n","Epoch 102/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1888 - accuracy: 0.9450\n","Epoch 102: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 10s 218ms/step - loss: 0.1888 - accuracy: 0.9450 - val_loss: 3.4785 - val_accuracy: 0.5866\n","Epoch 103/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1705 - accuracy: 0.9443\n","Epoch 103: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 9s 200ms/step - loss: 0.1705 - accuracy: 0.9443 - val_loss: 2.3614 - val_accuracy: 0.5753\n","Epoch 104/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1477 - accuracy: 0.9478\n","Epoch 104: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 9s 202ms/step - loss: 0.1477 - accuracy: 0.9478 - val_loss: 2.9495 - val_accuracy: 0.6207\n","Epoch 105/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1489 - accuracy: 0.9478\n","Epoch 105: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 10s 218ms/step - loss: 0.1489 - accuracy: 0.9478 - val_loss: 3.3340 - val_accuracy: 0.5895\n","Epoch 106/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1276 - accuracy: 0.9555\n","Epoch 106: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 11s 235ms/step - loss: 0.1276 - accuracy: 0.9555 - val_loss: 3.6848 - val_accuracy: 0.5994\n","Epoch 107/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1148 - accuracy: 0.9579\n","Epoch 107: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 10s 216ms/step - loss: 0.1148 - accuracy: 0.9579 - val_loss: 3.2218 - val_accuracy: 0.6151\n","Epoch 108/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1406 - accuracy: 0.9478\n","Epoch 108: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 9s 200ms/step - loss: 0.1406 - accuracy: 0.9478 - val_loss: 2.9630 - val_accuracy: 0.5952\n","Epoch 109/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1453 - accuracy: 0.9447\n","Epoch 109: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 10s 223ms/step - loss: 0.1453 - accuracy: 0.9447 - val_loss: 3.9218 - val_accuracy: 0.5938\n","Epoch 110/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1386 - accuracy: 0.9464\n","Epoch 110: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 9s 201ms/step - loss: 0.1386 - accuracy: 0.9464 - val_loss: 2.7648 - val_accuracy: 0.6307\n","Epoch 111/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1258 - accuracy: 0.9600\n","Epoch 111: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 9s 201ms/step - loss: 0.1258 - accuracy: 0.9600 - val_loss: 3.1565 - val_accuracy: 0.6179\n","Epoch 112/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1204 - accuracy: 0.9603\n","Epoch 112: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 9s 200ms/step - loss: 0.1204 - accuracy: 0.9603 - val_loss: 3.2940 - val_accuracy: 0.6179\n","Epoch 113/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1298 - accuracy: 0.9523\n","Epoch 113: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 9s 200ms/step - loss: 0.1298 - accuracy: 0.9523 - val_loss: 3.1203 - val_accuracy: 0.6278\n","Epoch 114/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0950 - accuracy: 0.9614\n","Epoch 114: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 10s 220ms/step - loss: 0.0950 - accuracy: 0.9614 - val_loss: 2.9001 - val_accuracy: 0.6094\n","Epoch 115/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0963 - accuracy: 0.9610\n","Epoch 115: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 9s 202ms/step - loss: 0.0963 - accuracy: 0.9610 - val_loss: 3.6989 - val_accuracy: 0.5852\n","Epoch 116/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0993 - accuracy: 0.9617\n","Epoch 116: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 9s 201ms/step - loss: 0.0993 - accuracy: 0.9617 - val_loss: 3.2666 - val_accuracy: 0.6065\n","Epoch 117/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1091 - accuracy: 0.9583\n","Epoch 117: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 9s 199ms/step - loss: 0.1091 - accuracy: 0.9583 - val_loss: 3.1662 - val_accuracy: 0.6094\n","Epoch 118/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1140 - accuracy: 0.9562\n","Epoch 118: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 10s 217ms/step - loss: 0.1140 - accuracy: 0.9562 - val_loss: 2.6686 - val_accuracy: 0.6307\n","Epoch 119/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.9625\n","Epoch 119: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 9s 200ms/step - loss: 0.0875 - accuracy: 0.9625 - val_loss: 2.8267 - val_accuracy: 0.6222\n","Epoch 120/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0933 - accuracy: 0.9642\n","Epoch 120: val_accuracy did not improve from 0.63210\n","45/45 [==============================] - 9s 198ms/step - loss: 0.0933 - accuracy: 0.9642 - val_loss: 3.4328 - val_accuracy: 0.6236\n","Epoch 121/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1055 - accuracy: 0.9593\n","Epoch 121: val_accuracy improved from 0.63210 to 0.64062, saving model to flowers.h5\n","45/45 [==============================] - 10s 221ms/step - loss: 0.1055 - accuracy: 0.9593 - val_loss: 3.2021 - val_accuracy: 0.6406\n","Epoch 122/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1063 - accuracy: 0.9645\n","Epoch 122: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 10s 220ms/step - loss: 0.1063 - accuracy: 0.9645 - val_loss: 2.9619 - val_accuracy: 0.6349\n","Epoch 123/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0981 - accuracy: 0.9593\n","Epoch 123: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 10s 217ms/step - loss: 0.0981 - accuracy: 0.9593 - val_loss: 3.0576 - val_accuracy: 0.6264\n","Epoch 124/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0955 - accuracy: 0.9642\n","Epoch 124: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 198ms/step - loss: 0.0955 - accuracy: 0.9642 - val_loss: 3.7357 - val_accuracy: 0.6193\n","Epoch 125/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1060 - accuracy: 0.9635\n","Epoch 125: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 198ms/step - loss: 0.1060 - accuracy: 0.9635 - val_loss: 3.1445 - val_accuracy: 0.6335\n","Epoch 126/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0895 - accuracy: 0.9670\n","Epoch 126: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 10s 218ms/step - loss: 0.0895 - accuracy: 0.9670 - val_loss: 4.3641 - val_accuracy: 0.5980\n","Epoch 127/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0891 - accuracy: 0.9638\n","Epoch 127: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 10s 213ms/step - loss: 0.0891 - accuracy: 0.9638 - val_loss: 3.3748 - val_accuracy: 0.6179\n","Epoch 128/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0791 - accuracy: 0.9683\n","Epoch 128: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 203ms/step - loss: 0.0791 - accuracy: 0.9683 - val_loss: 3.5751 - val_accuracy: 0.6406\n","Epoch 129/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0933 - accuracy: 0.9603\n","Epoch 129: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 201ms/step - loss: 0.0933 - accuracy: 0.9603 - val_loss: 2.9184 - val_accuracy: 0.6080\n","Epoch 130/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1075 - accuracy: 0.9548\n","Epoch 130: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 198ms/step - loss: 0.1075 - accuracy: 0.9548 - val_loss: 3.8258 - val_accuracy: 0.6193\n","Epoch 131/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0983 - accuracy: 0.9680\n","Epoch 131: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 200ms/step - loss: 0.0983 - accuracy: 0.9680 - val_loss: 3.4071 - val_accuracy: 0.6094\n","Epoch 132/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1231 - accuracy: 0.9530\n","Epoch 132: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 10s 213ms/step - loss: 0.1231 - accuracy: 0.9530 - val_loss: 3.4223 - val_accuracy: 0.6065\n","Epoch 133/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1127 - accuracy: 0.9590\n","Epoch 133: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 202ms/step - loss: 0.1127 - accuracy: 0.9590 - val_loss: 3.5819 - val_accuracy: 0.6136\n","Epoch 134/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0863 - accuracy: 0.9635\n","Epoch 134: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 203ms/step - loss: 0.0863 - accuracy: 0.9635 - val_loss: 3.0679 - val_accuracy: 0.6179\n","Epoch 135/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0882 - accuracy: 0.9694\n","Epoch 135: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 201ms/step - loss: 0.0882 - accuracy: 0.9694 - val_loss: 3.2474 - val_accuracy: 0.6122\n","Epoch 136/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0835 - accuracy: 0.9649\n","Epoch 136: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 10s 220ms/step - loss: 0.0835 - accuracy: 0.9649 - val_loss: 3.4205 - val_accuracy: 0.6278\n","Epoch 137/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0859 - accuracy: 0.9645\n","Epoch 137: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 202ms/step - loss: 0.0859 - accuracy: 0.9645 - val_loss: 3.1312 - val_accuracy: 0.6179\n","Epoch 138/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0867 - accuracy: 0.9614\n","Epoch 138: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 202ms/step - loss: 0.0867 - accuracy: 0.9614 - val_loss: 3.1410 - val_accuracy: 0.6293\n","Epoch 139/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0856 - accuracy: 0.9687\n","Epoch 139: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 200ms/step - loss: 0.0856 - accuracy: 0.9687 - val_loss: 4.0427 - val_accuracy: 0.6037\n","Epoch 140/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1113 - accuracy: 0.9704\n","Epoch 140: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 10s 216ms/step - loss: 0.1113 - accuracy: 0.9704 - val_loss: 3.9150 - val_accuracy: 0.5923\n","Epoch 141/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0801 - accuracy: 0.9670\n","Epoch 141: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 10s 234ms/step - loss: 0.0801 - accuracy: 0.9670 - val_loss: 3.0797 - val_accuracy: 0.6321\n","Epoch 142/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0818 - accuracy: 0.9704\n","Epoch 142: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 203ms/step - loss: 0.0818 - accuracy: 0.9704 - val_loss: 3.6823 - val_accuracy: 0.6080\n","Epoch 143/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0905 - accuracy: 0.9701\n","Epoch 143: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 204ms/step - loss: 0.0905 - accuracy: 0.9701 - val_loss: 3.6460 - val_accuracy: 0.6250\n","Epoch 144/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0831 - accuracy: 0.9673\n","Epoch 144: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 203ms/step - loss: 0.0831 - accuracy: 0.9673 - val_loss: 3.0364 - val_accuracy: 0.6250\n","Epoch 145/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0778 - accuracy: 0.9718\n","Epoch 145: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 10s 215ms/step - loss: 0.0778 - accuracy: 0.9718 - val_loss: 3.4490 - val_accuracy: 0.6236\n","Epoch 146/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0956 - accuracy: 0.9656\n","Epoch 146: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 202ms/step - loss: 0.0956 - accuracy: 0.9656 - val_loss: 3.2966 - val_accuracy: 0.5966\n","Epoch 147/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0854 - accuracy: 0.9659\n","Epoch 147: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 204ms/step - loss: 0.0854 - accuracy: 0.9659 - val_loss: 2.9962 - val_accuracy: 0.6222\n","Epoch 148/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0941 - accuracy: 0.9697\n","Epoch 148: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 201ms/step - loss: 0.0941 - accuracy: 0.9697 - val_loss: 3.5071 - val_accuracy: 0.6037\n","Epoch 149/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0898 - accuracy: 0.9663\n","Epoch 149: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 200ms/step - loss: 0.0898 - accuracy: 0.9663 - val_loss: 3.0708 - val_accuracy: 0.6278\n","Epoch 150/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0815 - accuracy: 0.9697\n","Epoch 150: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 10s 215ms/step - loss: 0.0815 - accuracy: 0.9697 - val_loss: 2.8096 - val_accuracy: 0.6378\n","Epoch 151/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0763 - accuracy: 0.9694\n","Epoch 151: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 203ms/step - loss: 0.0763 - accuracy: 0.9694 - val_loss: 3.2398 - val_accuracy: 0.6080\n","Epoch 152/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9708\n","Epoch 152: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 201ms/step - loss: 0.0788 - accuracy: 0.9708 - val_loss: 3.3734 - val_accuracy: 0.6250\n","Epoch 153/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0836 - accuracy: 0.9659\n","Epoch 153: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 204ms/step - loss: 0.0836 - accuracy: 0.9659 - val_loss: 3.0982 - val_accuracy: 0.6278\n","Epoch 154/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0836 - accuracy: 0.9704\n","Epoch 154: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 10s 212ms/step - loss: 0.0836 - accuracy: 0.9704 - val_loss: 3.8933 - val_accuracy: 0.6065\n","Epoch 155/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0776 - accuracy: 0.9701\n","Epoch 155: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 201ms/step - loss: 0.0776 - accuracy: 0.9701 - val_loss: 3.1602 - val_accuracy: 0.6250\n","Epoch 156/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0715 - accuracy: 0.9739\n","Epoch 156: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 199ms/step - loss: 0.0715 - accuracy: 0.9739 - val_loss: 3.8527 - val_accuracy: 0.6151\n","Epoch 157/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0822 - accuracy: 0.9694\n","Epoch 157: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 9s 201ms/step - loss: 0.0822 - accuracy: 0.9694 - val_loss: 3.4753 - val_accuracy: 0.6222\n","Epoch 158/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0764 - accuracy: 0.9697\n","Epoch 158: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 10s 221ms/step - loss: 0.0764 - accuracy: 0.9697 - val_loss: 3.3905 - val_accuracy: 0.6349\n","Epoch 159/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9725\n","Epoch 159: val_accuracy did not improve from 0.64062\n","45/45 [==============================] - 10s 232ms/step - loss: 0.0787 - accuracy: 0.9725 - val_loss: 3.6398 - val_accuracy: 0.6165\n","Epoch 160/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0875 - accuracy: 0.9687\n","Epoch 160: val_accuracy improved from 0.64062 to 0.64915, saving model to flowers.h5\n","45/45 [==============================] - 9s 206ms/step - loss: 0.0875 - accuracy: 0.9687 - val_loss: 3.7503 - val_accuracy: 0.6491\n","Epoch 161/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0936 - accuracy: 0.9649\n","Epoch 161: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 9s 199ms/step - loss: 0.0936 - accuracy: 0.9649 - val_loss: 3.8273 - val_accuracy: 0.6207\n","Epoch 162/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0876 - accuracy: 0.9645\n","Epoch 162: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 9s 202ms/step - loss: 0.0876 - accuracy: 0.9645 - val_loss: 3.0078 - val_accuracy: 0.6392\n","Epoch 163/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0790 - accuracy: 0.9690\n","Epoch 163: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 10s 226ms/step - loss: 0.0790 - accuracy: 0.9690 - val_loss: 3.1613 - val_accuracy: 0.6477\n","Epoch 164/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0742 - accuracy: 0.9722\n","Epoch 164: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 10s 221ms/step - loss: 0.0742 - accuracy: 0.9722 - val_loss: 3.6214 - val_accuracy: 0.6264\n","Epoch 165/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0780 - accuracy: 0.9683\n","Epoch 165: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 9s 200ms/step - loss: 0.0780 - accuracy: 0.9683 - val_loss: 2.9634 - val_accuracy: 0.6435\n","Epoch 166/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0726 - accuracy: 0.9718\n","Epoch 166: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 9s 199ms/step - loss: 0.0726 - accuracy: 0.9718 - val_loss: 3.1797 - val_accuracy: 0.6151\n","Epoch 167/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0869 - accuracy: 0.9732\n","Epoch 167: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 10s 214ms/step - loss: 0.0869 - accuracy: 0.9732 - val_loss: 3.4748 - val_accuracy: 0.6207\n","Epoch 168/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0703 - accuracy: 0.9711\n","Epoch 168: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 9s 200ms/step - loss: 0.0703 - accuracy: 0.9711 - val_loss: 3.5804 - val_accuracy: 0.6009\n","Epoch 169/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0787 - accuracy: 0.9708\n","Epoch 169: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 9s 201ms/step - loss: 0.0787 - accuracy: 0.9708 - val_loss: 2.8008 - val_accuracy: 0.6136\n","Epoch 170/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0710 - accuracy: 0.9708\n","Epoch 170: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 9s 200ms/step - loss: 0.0710 - accuracy: 0.9708 - val_loss: 3.2859 - val_accuracy: 0.6236\n","Epoch 171/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9680\n","Epoch 171: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 9s 201ms/step - loss: 0.0802 - accuracy: 0.9680 - val_loss: 3.5008 - val_accuracy: 0.6179\n","Epoch 172/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0697 - accuracy: 0.9736\n","Epoch 172: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 10s 212ms/step - loss: 0.0697 - accuracy: 0.9736 - val_loss: 3.1166 - val_accuracy: 0.6293\n","Epoch 173/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0788 - accuracy: 0.9708\n","Epoch 173: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 9s 202ms/step - loss: 0.0788 - accuracy: 0.9708 - val_loss: 3.9836 - val_accuracy: 0.6236\n","Epoch 174/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0922 - accuracy: 0.9701\n","Epoch 174: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 9s 200ms/step - loss: 0.0922 - accuracy: 0.9701 - val_loss: 2.8422 - val_accuracy: 0.6307\n","Epoch 175/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0706 - accuracy: 0.9711\n","Epoch 175: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 9s 202ms/step - loss: 0.0706 - accuracy: 0.9711 - val_loss: 3.3276 - val_accuracy: 0.6278\n","Epoch 176/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0857 - accuracy: 0.9701\n","Epoch 176: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 10s 213ms/step - loss: 0.0857 - accuracy: 0.9701 - val_loss: 3.2631 - val_accuracy: 0.6151\n","Epoch 177/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0831 - accuracy: 0.9687\n","Epoch 177: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 10s 211ms/step - loss: 0.0831 - accuracy: 0.9687 - val_loss: 3.4327 - val_accuracy: 0.6122\n","Epoch 178/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0746 - accuracy: 0.9725\n","Epoch 178: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 9s 200ms/step - loss: 0.0746 - accuracy: 0.9725 - val_loss: 3.8501 - val_accuracy: 0.5724\n","Epoch 179/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0718 - accuracy: 0.9743\n","Epoch 179: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 9s 200ms/step - loss: 0.0718 - accuracy: 0.9743 - val_loss: 3.4317 - val_accuracy: 0.6278\n","Epoch 180/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0796 - accuracy: 0.9718\n","Epoch 180: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 9s 199ms/step - loss: 0.0796 - accuracy: 0.9718 - val_loss: 3.5632 - val_accuracy: 0.6193\n","Epoch 181/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0915 - accuracy: 0.9704\n","Epoch 181: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 10s 214ms/step - loss: 0.0915 - accuracy: 0.9704 - val_loss: 3.8252 - val_accuracy: 0.6023\n","Epoch 182/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0651 - accuracy: 0.9729\n","Epoch 182: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 9s 200ms/step - loss: 0.0651 - accuracy: 0.9729 - val_loss: 3.8417 - val_accuracy: 0.5952\n","Epoch 183/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0918 - accuracy: 0.9715\n","Epoch 183: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 9s 202ms/step - loss: 0.0918 - accuracy: 0.9715 - val_loss: 3.2477 - val_accuracy: 0.6023\n","Epoch 184/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0591 - accuracy: 0.9763\n","Epoch 184: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 9s 202ms/step - loss: 0.0591 - accuracy: 0.9763 - val_loss: 3.4701 - val_accuracy: 0.6179\n","Epoch 185/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0761 - accuracy: 0.9736\n","Epoch 185: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 9s 201ms/step - loss: 0.0761 - accuracy: 0.9736 - val_loss: 3.4768 - val_accuracy: 0.6136\n","Epoch 186/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0753 - accuracy: 0.9743\n","Epoch 186: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 10s 231ms/step - loss: 0.0753 - accuracy: 0.9743 - val_loss: 3.3540 - val_accuracy: 0.6122\n","Epoch 187/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0802 - accuracy: 0.9715\n","Epoch 187: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 9s 199ms/step - loss: 0.0802 - accuracy: 0.9715 - val_loss: 3.8516 - val_accuracy: 0.6009\n","Epoch 188/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0735 - accuracy: 0.9715\n","Epoch 188: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 9s 201ms/step - loss: 0.0735 - accuracy: 0.9715 - val_loss: 3.8918 - val_accuracy: 0.6009\n","Epoch 189/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1936 - accuracy: 0.9555\n","Epoch 189: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 10s 232ms/step - loss: 0.1936 - accuracy: 0.9555 - val_loss: 2.4335 - val_accuracy: 0.6136\n","Epoch 190/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1537 - accuracy: 0.9576\n","Epoch 190: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 9s 201ms/step - loss: 0.1537 - accuracy: 0.9576 - val_loss: 2.9999 - val_accuracy: 0.6065\n","Epoch 191/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1637 - accuracy: 0.9562\n","Epoch 191: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 9s 201ms/step - loss: 0.1637 - accuracy: 0.9562 - val_loss: 3.2665 - val_accuracy: 0.5909\n","Epoch 192/200\n","45/45 [==============================] - ETA: 0s - loss: 0.1654 - accuracy: 0.9565\n","Epoch 192: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 11s 238ms/step - loss: 0.1654 - accuracy: 0.9565 - val_loss: 3.2152 - val_accuracy: 0.5824\n","Epoch 193/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0812 - accuracy: 0.9715\n","Epoch 193: val_accuracy did not improve from 0.64915\n","45/45 [==============================] - 9s 201ms/step - loss: 0.0812 - accuracy: 0.9715 - val_loss: 3.1291 - val_accuracy: 0.6264\n","Epoch 194/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0847 - accuracy: 0.9690\n","Epoch 194: val_accuracy improved from 0.64915 to 0.65199, saving model to flowers.h5\n","45/45 [==============================] - 10s 223ms/step - loss: 0.0847 - accuracy: 0.9690 - val_loss: 2.7738 - val_accuracy: 0.6520\n","Epoch 195/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0749 - accuracy: 0.9694\n","Epoch 195: val_accuracy did not improve from 0.65199\n","45/45 [==============================] - 9s 198ms/step - loss: 0.0749 - accuracy: 0.9694 - val_loss: 3.3670 - val_accuracy: 0.6378\n","Epoch 196/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0846 - accuracy: 0.9711\n","Epoch 196: val_accuracy did not improve from 0.65199\n","45/45 [==============================] - 9s 200ms/step - loss: 0.0846 - accuracy: 0.9711 - val_loss: 3.7861 - val_accuracy: 0.6349\n","Epoch 197/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0903 - accuracy: 0.9694\n","Epoch 197: val_accuracy did not improve from 0.65199\n","45/45 [==============================] - 9s 212ms/step - loss: 0.0903 - accuracy: 0.9694 - val_loss: 3.3555 - val_accuracy: 0.6335\n","Epoch 198/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0711 - accuracy: 0.9704\n","Epoch 198: val_accuracy did not improve from 0.65199\n","45/45 [==============================] - 9s 201ms/step - loss: 0.0711 - accuracy: 0.9704 - val_loss: 3.6406 - val_accuracy: 0.6207\n","Epoch 199/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0698 - accuracy: 0.9718\n","Epoch 199: val_accuracy did not improve from 0.65199\n","45/45 [==============================] - 9s 202ms/step - loss: 0.0698 - accuracy: 0.9718 - val_loss: 3.2579 - val_accuracy: 0.6250\n","Epoch 200/200\n","45/45 [==============================] - ETA: 0s - loss: 0.0636 - accuracy: 0.9760\n","Epoch 200: val_accuracy did not improve from 0.65199\n","45/45 [==============================] - 10s 221ms/step - loss: 0.0636 - accuracy: 0.9760 - val_loss: 3.9526 - val_accuracy: 0.6193\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f8b2289d3d0>"]},"metadata":{},"execution_count":21}]},{"cell_type":"code","source":["#Clear any previous model from memory\n","tf.keras.backend.clear_session()\n","\n","#Initialize model\n","model = tf.keras.models.Sequential()\n","\n","#normalize data\n","model.add(tf.keras.layers.BatchNormalization(input_shape=(100,100,3,)))\n","\n","#Add Conv Layer\n","model.add(tf.keras.layers.Conv2D(32,\n","                                 kernel_size=(3,3),\n","                                 activation='relu'))\n","\n","#normalize data\n","model.add(tf.keras.layers.BatchNormalization())\n","\n","\n","#Add Dropout\n","model.add(tf.keras.layers.Dropout(0.35))\n","\n","#Add Conv Layer\n","model.add(tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'))\n","\n","#normalize data\n","model.add(tf.keras.layers.BatchNormalization())\n","\n","#Add Max Pool layer\n","model.add(tf.keras.layers.MaxPool2D(pool_size=(2,2)))\n","\n","#Add Dense Layers after flattening the data\n","model.add(tf.keras.layers.Flatten())\n","model.add(tf.keras.layers.Dense(128, activation='relu'))\n","\n","#Add Dropout\n","model.add(tf.keras.layers.Dropout(0.25))\n","\n","#Add Output Layer\n","model.add(tf.keras.layers.Dense(5, activation='softmax'))"],"metadata":{"id":"lZmJo1sbXiOp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"S9njmNCkdotB","executionInfo":{"status":"ok","timestamp":1679743249153,"user_tz":-330,"elapsed":633,"user":{"displayName":"Rajeev Kumar","userId":"10567937244174773728"}},"outputId":"ed8d48ef-91cb-45a9-efc4-61f5a82fcfa5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," batch_normalization (BatchN  (None, 100, 100, 3)      12        \n"," ormalization)                                                   \n","                                                                 \n"," conv2d (Conv2D)             (None, 98, 98, 32)        896       \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 98, 98, 32)       128       \n"," hNormalization)                                                 \n","                                                                 \n"," dropout (Dropout)           (None, 98, 98, 32)        0         \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 96, 96, 64)        18496     \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 96, 96, 64)       256       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 48, 48, 64)       0         \n"," )                                                               \n","                                                                 \n"," flatten (Flatten)           (None, 147456)            0         \n","                                                                 \n"," dense (Dense)               (None, 128)               18874496  \n","                                                                 \n"," dropout_1 (Dropout)         (None, 128)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 645       \n","                                                                 \n","=================================================================\n","Total params: 18,894,929\n","Trainable params: 18,894,731\n","Non-trainable params: 198\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gcvtY4SDXm_y","executionInfo":{"status":"ok","timestamp":1679742590796,"user_tz":-330,"elapsed":596,"user":{"displayName":"Rajeev Kumar","userId":"10567937244174773728"}},"outputId":"fd1b8f27-47b3-4b97-fc18-f4498f583527"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," batch_normalization (BatchN  (None, 60, 60, 3)        12        \n"," ormalization)                                                   \n","                                                                 \n"," conv2d (Conv2D)             (None, 58, 58, 32)        896       \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 58, 58, 32)       128       \n"," hNormalization)                                                 \n","                                                                 \n"," conv2d_1 (Conv2D)           (None, 56, 56, 64)        18496     \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 56, 56, 64)       256       \n"," hNormalization)                                                 \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 28, 28, 64)       0         \n"," )                                                               \n","                                                                 \n"," flatten (Flatten)           (None, 50176)             0         \n","                                                                 \n"," dense (Dense)               (None, 128)               6422656   \n","                                                                 \n"," dropout (Dropout)           (None, 128)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 5)                 645       \n","                                                                 \n","=================================================================\n","Total params: 6,443,089\n","Trainable params: 6,442,891\n","Non-trainable params: 198\n","_________________________________________________________________\n"]}]}]}