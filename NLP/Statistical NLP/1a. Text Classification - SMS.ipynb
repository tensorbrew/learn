{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":["rKqe-5Lpwj7E","c2h-db6Zw00i","pERZB-yYcMnQ","8_WO6pSdvE3o","Pn-BuPr7wj7u","4o12llmedMEi","0fAMugIPwj-9","YIHaWAagwj_H"]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"rKqe-5Lpwj7E"},"source":["### 1. SMS Data Exploration"]},{"cell_type":"markdown","metadata":{"id":"SgZQ20psaJJX"},"source":["SMS data is available as CSV file along with class material. In the code below, we are copying the from Google drive."]},{"cell_type":"code","metadata":{"id":"evvJtcnyaeO_"},"source":["from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o84tU4KBruJK"},"source":["import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WcQh6U1-aeOh"},"source":["# read file into pandas using a relative path. Please change the path as needed\n","sms_df = pd.read_table('/gdrive/My Drive/ML Content/Statistical NLP/Notebooks/data/sms.tsv', header=None, names=['label', 'message'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZOgLs2FvbRx-"},"source":["#Total number of SMS\n","sms_df.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qDN-bzy9aBAz"},"source":["#Check the contents of dataframe\n","sms_df.sample(n=15)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PGm3aYgGt7a9"},"source":["sms_df.loc[0, 'message']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EAimr_7Trmk0"},"source":["#Spam vs ham\n","sms_df.groupby('label').count()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OuxSfoEcU6s1"},"source":["Null Accuracy - Predicting"]},{"cell_type":"code","metadata":{"id":"4AtiyLOW0Fp7"},"source":["4825/5572"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MvIX0dRgbbzp"},"source":["#Check out SMS messages which is legitimate - ham\n","msg_num = np.random.randint(0, sms_df.shape[0])\n","print(sms_df.loc[msg_num, 'label'], ':', sms_df.loc[msg_num, 'message'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EGvWX0DvY48J"},"source":["#Check out SMS messages which is a SPAM\n","print(sms_df.loc[1734, 'label'], ':', sms_df.loc[1734, 'message'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A705u9O1uor6"},"source":["#Checkout missing values\n","sms_df.isnull().sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Oy8Dja0ExL-B"},"source":["# convert label to a numerical variable\n","sms_df['label_num'] = sms_df.label.map({'ham':0, 'spam':1})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HBd1Q7HJxOox"},"source":["#We should have label_num column in dataframe\n","sms_df.sample(n=15)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c2h-db6Zw00i"},"source":["### 2. Create Training & Test Dataset"]},{"cell_type":"code","metadata":{"id":"v-wdhCspw4lE"},"source":["from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4mY8_xP6w9DW"},"source":["# split X and y into training and testing sets\n","sms_train, sms_test, y_train, y_test = train_test_split(sms_df.message,\n","                                                        sms_df.label_num,\n","                                                        random_state=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sms_train.reset_index(inplace=True, drop=True)\n","sms_test.reset_index(inplace=True, drop=True)\n","y_train.reset_index(inplace=True, drop=True)\n","y_test.reset_index(inplace=True, drop=True)"],"metadata":{"id":"F2Um35GQ3OhN"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"axydR7nwxd5c"},"source":["#Traing data\n","print(sms_train.shape)\n","print(y_train.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cMpz2JpNxjT1"},"source":["#Test Data\n","print(sms_test.shape)\n","print(y_test.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pERZB-yYcMnQ"},"source":["### 3. Tokenization & Vectorization"]},{"cell_type":"markdown","metadata":{"id":"HG5KU5JVwj7k"},"source":["Using **CountVectorizer**, to get numeric features."]},{"cell_type":"code","metadata":{"id":"o-EAh3Lict_N"},"source":["# import and instantiate CountVectorizer (with the default parameters)\n","from sklearn.feature_extraction.text import CountVectorizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["cvect = CountVectorizer()"],"metadata":{"id":"0e8qWBaQ3nP-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["sms_train.head()"],"metadata":{"id":"0z36rRaI3tc2"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wOIKxD1pdEg3"},"source":["#Feed SMS data to CountVectorizer\n","cvect.fit(sms_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CF-9vF0ZxQO6"},"source":["#Check the vocablury size\n","len(cvect.vocabulary_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H4t9gegJd74n"},"source":["#What is there in the vocabulary\n","cvect.vocabulary_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UK1zjGfHuP4U"},"source":["Build Document-term Matrix (DTM)"]},{"cell_type":"code","metadata":{"id":"DrsvbF1XdefF"},"source":["#Convert Training SMS messages into Count Vectors\n","X_train_ct = cvect.transform(sms_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7qDPMLuFduAx"},"source":["#Size of Document Term Matrix\n","X_train_ct.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8vDpvDjPxi7c"},"source":["sms_train[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FR2qtdrMeXkL"},"source":["#Let's check the first record\n","X_train_ct[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YaTnzUIMeoMt"},"source":["#What's there in sparse matrix\n","print(X_train_ct[0:1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#X_train_ct[0:1].todense()"],"metadata":{"id":"QT1J9xxl5dQI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"twoJfGyXwj80"},"source":["From the [scikit-learn documentation](http://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction):\n","\n","> As most documents will typically use a very small subset of the words used in the corpus, the resulting matrix will have **many feature values that are zeros** (typically more than 99% of them).\n","\n","> For instance, a collection of 10,000 short text documents (such as emails) will use a vocabulary with a size in the order of 100,000 unique words in total while each document will use 100 to 1000 unique words individually.\n","\n","> In order to be able to **store such a matrix in memory** but also to **speed up operations**, implementations will typically use a **sparse representation** such as the implementations available in the `scipy.sparse` package."]},{"cell_type":"markdown","metadata":{"id":"9zo2EwHYzAUr"},"source":["Convert Test SMS also in numerical features"]},{"cell_type":"code","metadata":{"id":"HX8ht6-tzDuu"},"source":["X_test_ct = cvect.transform(sms_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zWjwfp9AzKL_"},"source":["X_test_ct.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8_WO6pSdvE3o"},"source":["### 4. Building an SMS Classifier"]},{"cell_type":"markdown","metadata":{"id":"HEYiNTZPx_UJ"},"source":["Let's first try K-Nearest Neigbour algorithm"]},{"cell_type":"code","metadata":{"id":"f4ysA2I8wj7l"},"source":["from sklearn.neighbors import KNeighborsClassifier"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"41v1PorCyHHj"},"source":["# instantiate the model (with the default parameters)\n","knn = KNeighborsClassifier()\n","\n","# fit the model with data (occurs in-place)\n","knn.fit(X_train_ct, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lzexfE7Wwj7p"},"source":["Evaluation on Test Dataset"]},{"cell_type":"code","metadata":{"id":"uQOrAY7gy2Gp"},"source":["from sklearn import metrics"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6jJ1fLXAwj7q"},"source":["#Calculate accuracy on Test Dataset\n","metrics.accuracy_score(y_test, knn.predict(X_test_ct))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zjw9Iw7Yz2za"},"source":["#Calculate accuracy on Training Dataset\n","metrics.accuracy_score(y_train, knn.predict(X_train_ct))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hyJmh5BuzZhW"},"source":["We can build Classifier using other algorithms e.g SVM"]},{"cell_type":"code","metadata":{"id":"G0k6aBRTzY6i"},"source":["from sklearn.svm import SVC"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DxLV-8oZbHQs"},"source":["#Train an SVM with default parameters\n","svc = SVC()\n","svc.fit(X_train_ct, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1Oq4hVfHbWM0"},"source":["#Calculate accuracy on Test Dataset\n","metrics.accuracy_score(y_test, svc.predict(X_test_ct))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Calculate accuracy on Train Dataset\n","metrics.accuracy_score(y_train, svc.predict(X_train_ct))"],"metadata":{"id":"tA-2UWjy9Y7z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier"],"metadata":{"id":"Vn9PznWnMwd8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rf_model = RandomForestClassifier()\n","rf_model.fit(X_train_ct, y_train)"],"metadata":{"id":"98M3ymSWNC4e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Calculate accuracy on Test Dataset\n","metrics.accuracy_score(y_test, rf_model.predict(X_test_ct))"],"metadata":{"id":"E1fW8Q0vNNoD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Calculate accuracy on Train Dataset\n","metrics.accuracy_score(y_train, rf_model.predict(X_train_ct))"],"metadata":{"id":"kL2NYck9-ALQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Pn-BuPr7wj7u"},"source":["### 5. Using TF-IDF Vectorizer"]},{"cell_type":"code","metadata":{"id":"_h06uXvYwj7u"},"source":["# import and instantiate TF-IDF Vectorizer (with the default parameters)\n","from sklearn.feature_extraction.text import TfidfVectorizer"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tvect = TfidfVectorizer()"],"metadata":{"id":"Cu2yq5Hh-dk8"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8bAGvD0Ewj7y"},"source":["#Feed SMS data to CountVectorizer\n","tvect.fit(sms_train)\n","\n","#Check the vocablury size\n","len(tvect.vocabulary_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"exeOGALacCb4"},"source":["#Convert Training SMS messages into numerical values\n","X_train_tfidf = tvect.transform(sms_train)\n","\n","X_train_tfidf.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VcJwt_UscVWO"},"source":["#Check first example\n","print(X_train_tfidf[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0vy_edXec0vP"},"source":["#Convert Test SMSes also to tf-idf vectors\n","X_test_tfidf = tvect.transform(sms_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-c77Chsvcg5d"},"source":["Build an Random Forest"]},{"cell_type":"code","metadata":{"id":"eGxYIu00ca-s"},"source":["rf_model_tf = RandomForestClassifier()\n","rf_model_tf.fit(X_train_tfidf, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AeXoA575cr3A"},"source":["#Calculate accuracy on Test Dataset\n","metrics.accuracy_score(y_test, rf_model_tf.predict(X_test_tfidf))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4o12llmedMEi"},"source":["### 6. TF-IDF with ngram"]},{"cell_type":"code","metadata":{"id":"XJnl3-kXdPuT"},"source":["#Use ngrams of length upto 2 words\n","tvect_ngram = TfidfVectorizer(ngram_range=(1,2)) #Tokens can be made of 1 word or 2 words"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#sms_df.head()"],"metadata":{"id":"qOZFAxJi7wTP"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"amKZ3BmGdr4x"},"source":["#Feed SMS data to CountVectorizer\n","tvect_ngram.fit(sms_df.message)\n","\n","#Check the vocablury size\n","len(tvect_ngram.vocabulary_)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xVr8GXs91ME_"},"source":["The movie was awesome\n","\n","Words as tokens = \"The\", \"movie\", \"was\", awesome\"\n","\n","ngrams (1,2) -> \"The\", \"movie\", \"was\", awesome\", \"The movie\", \"movie was\", \"was awesome\""]},{"cell_type":"code","metadata":{"id":"t0gI6RN-dz3i"},"source":["tvect_ngram.vocabulary_"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t3__SHIJeCKI"},"source":["#Convert Training SMS messages into numerical values\n","X_train_tfidf_ngram = tvect_ngram.transform(sms_train)\n","\n","X_train_tfidf_ngram.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h7nxDtrUVN0R"},"source":["rf_model_tf_ngram = RandomForestClassifier()\n","rf_model_tf_ngram.fit(X_train_tfidf_ngram, y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"epvazfQbVYGu"},"source":["#Calculate accuracy on Test Dataset\n","metrics.accuracy_score(y_test, rf_model_tf_ngram.predict(tvect_ngram.transform(sms_test)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Calculate accuracy on Train Dataset\n","metrics.accuracy_score(y_train, rf_model_tf_ngram.predict(X_train_tfidf_ngram))"],"metadata":{"id":"TpGqFRktASDl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bB6X2lLTwj9F"},"source":["**Summary:**\n","\n","- `vect.fit(train)` **learns the vocabulary** of the training data\n","- `vect.transform(train)` uses the **fitted vocabulary** to build a document-term matrix from the training data\n","- `vect.transform(test)` uses the **fitted vocabulary** to build a document-term matrix from the testing data and **ignores tokens** it hasn't seen before"]},{"cell_type":"markdown","metadata":{"id":"rE6sCTxjhPxR"},"source":["### 7. Building a Deep Learning Model"]},{"cell_type":"code","metadata":{"id":"pXcQt_TchaNW"},"source":["import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MofkwztZheaj"},"source":["We will use CountVectorizer features in this case. This can be replaced by TF-IDF features"]},{"cell_type":"code","metadata":{"id":"Qwt45bfbhcur"},"source":["#Start building a Keras Sequential Model\n","tf.keras.backend.clear_session()\n","model = tf.keras.Sequential()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jmYjSF9Mh11J"},"source":["#Add hidden layers\n","model.add(tf.keras.layers.Dense(100, activation='relu', input_shape=(len(tvect.vocabulary_),)))\n","model.add(tf.keras.layers.Dropout(0.4))\n","model.add(tf.keras.layers.Dense(50, activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.4))\n","\n","#Add Output layer\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MUPy_5VIiUG9"},"source":["#Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Yg4ZiI2AJ3mV"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X_train_ct.shape"],"metadata":{"id":"Z2p5Yuf-WwqR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.fit(X_train_ct.todense(), y_train,\n","           validation_data=(X_test_ct.todense(), y_test),\n","           epochs=10, batch_size=32)"],"metadata":{"id":"_jVvlrGuWlIH"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nigCTrwjXeWM"},"source":["X_train_ct.todense()[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Cn8kdNu3YUek"},"source":["print(X_train_ct[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Start building a Keras Sequential Model\n","tf.keras.backend.clear_session()\n","model = tf.keras.Sequential()\n","\n","#Add hidden layers\n","model.add(tf.keras.layers.Reshape((7450,1), input_shape=(len(tvect.vocabulary_),)))\n","model.add(tf.keras.layers.Conv1D(100, kernel_size=(3),activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.4))\n","model.add(tf.keras.layers.Conv1D(50, kernel_size=(3), activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.4))\n","\n","\n","#Add Output layer\n","model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n","\n","#Compile the model\n","model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])"],"metadata":{"id":"wcVYdFeWewMa"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zH-LnFyXiXqc"},"source":["model.fit(X_train_ct.todense(), y_train,\n","           validation_data=(X_test_ct.todense(), y_test),\n","           epochs=10,\n","           batch_size=32)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model.summary()"],"metadata":{"id":"i5bUojE-giFY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0fAMugIPwj-9"},"source":["### 8. Controlling Vocabulary size\n","\n","Thus far, we have been using the default parameters of [CountVectorizer](http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html):"]},{"cell_type":"code","source":["cvect.vocabulary_"],"metadata":{"id":"KonkP7jUKo-M"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9CbGtaqRwj--"},"source":["# show default parameters for CountVectorizer (TFIDF will have similar parameters)\n","?cvect"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AWehIREdwj-_"},"source":["However, the vectorizer is worth tuning, just like a model is worth tuning! Here are a few parameters that you might want to tune:\n","\n","- **stop_words:** string {'english'}, list, or None (default)\n","    - If 'english', a built-in stop word list for English is used.\n","    - If a list, that list is assumed to contain stop words, all of which will be removed from the resulting tokens.\n","    - If None, no stop words will be used."]},{"cell_type":"code","metadata":{"id":"DkCZFIKFwj_A"},"source":["# remove English stop words\n","vect = CountVectorizer(stop_words='english')\n","vect.fit(sms_train)\n","len(vect.get_feature_names_out())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZAhoUBLRwj_A"},"source":["- **ngram_range:** tuple (min_n, max_n), default=(1, 1)\n","    - The lower and upper boundary of the range of n-values for different n-grams to be extracted.\n","    - All values of n such that min_n <= n <= max_n will be used."]},{"cell_type":"code","metadata":{"id":"zvexuCwFwj_B"},"source":["# include 1-grams, 2-grams and 3-grams\n","vect = CountVectorizer(ngram_range=(1, 3))\n","vect.fit(sms_train)\n","len(vect.get_feature_names())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xzM7sYXHwj_D"},"source":["- **max_df:** float in range [0.0, 1.0] or int, default=1.0\n","    - When building the vocabulary, ignore terms that have a document frequency strictly higher than the given threshold (corpus-specific stop words).\n","    - If float, the parameter represents a proportion of documents.\n","    - If integer, the parameter represents an absolute count."]},{"cell_type":"code","metadata":{"id":"5QB_5WAnwj_D"},"source":["# ignore terms that appear in more than 50% of the documents\n","vect = CountVectorizer(max_df=0.5)\n","vect.fit(sms_train)\n","len(vect.get_feature_names_out())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0lPcHFE1s8Aa"},"source":["- **min_df:** int, default=1\n","\n","\n","> Defines, at a minimum, how many documents a word should appear before it is included in Vocablury\n"]},{"cell_type":"code","metadata":{"id":"vHC-oizLwj_F"},"source":["# only keep terms that appear in at least 2 documents\n","vect = CountVectorizer(min_df=2)\n","vect.fit(sms_train)\n","len(vect.get_feature_names_out())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4FsdJ9FBEIlY"},"source":["vect.vocabulary_"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bsmJkEbptdto"},"source":["- **max_features**: int or None, default=None\n","\n","\n","> Maximum size of vocabulary. None means no hard limit.\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"AV31mVnws27q"},"source":["# only keep terms that appear in at least 2 documents, but maximum vocablury is restricted to 2000 words\n","vect = CountVectorizer(min_df=2, max_features=2000)\n","vect.fit(sms_train)\n","len(vect.get_feature_names_out())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9RREbq_xwj_G"},"source":["**Guidelines for tuning Vectorizer:**\n","\n","- Use your knowledge of the **problem** and the **text**\n","- **Experiment**, and let the data tell you the best approach!\n","- Quiet often, number of features are limited by amount of RAM/Compute available."]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"YIHaWAagwj_H"},"source":["### Word Cloud"]},{"cell_type":"code","metadata":{"id":"SAK6X0-jwj_H"},"source":["import matplotlib.pyplot as plt # visualization\n","from wordcloud import WordCloud"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X5ix7Va-wj_M"},"source":["# Define wordcloud function from wordcloud library.\n","wc = WordCloud()\n","wc.generate(str(sms_df['message']))\n","# declare our figure\n","plt.figure(figsize=(20,10), facecolor='k')\n","# add title to the graph\n","plt.title(\"Most frequent words in SMS dataset\", fontsize=40, color='white')\n","plt.imshow(wc)\n","plt.show()"],"execution_count":null,"outputs":[]}]}