{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"JoJPolOgWGG4"},"source":["### Load tensorflow"]},{"cell_type":"code","metadata":{"id":"zlt_1eHPWGG6"},"source":["import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VzxABbMJWGG-"},"source":["### Read the data\n","<font size=\"2\">Data for this exercise can be downloaded from http://www.manythings.org/anki/</font>"]},{"cell_type":"code","metadata":{"id":"ommjMLuF75Dk"},"source":["!ls"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1vDoJM0zWGG_"},"source":["#You can use wget to download the file directly\n","!wget http://www.manythings.org/anki/hin-eng.zip --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A8soXgFrWGHB"},"source":["import zipfile\n","import io\n","\n","#Read the zip file\n","zf = zipfile.ZipFile('hin-eng.zip', 'r')\n","\n","#Extract data from zip file\n","data = ''\n","with zf.open('hin.txt') as readfile:\n","  for line in io.TextIOWrapper(readfile, 'utf-8'):\n","    data += line"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SQFmYMvhWGHE"},"source":["data[400:500]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lx5AWe-0WGHT"},"source":["\n","### Extract Source and Target Language pairs"]},{"cell_type":"code","metadata":{"id":"mjma2IuuWGHU"},"source":["#Split by newline character\n","data =  data.split('\\n')\n","\n","#Show some Data\n","data[100:105]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Af9jau2XWGHX"},"source":["len(data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A4ow84XDWGHb"},"source":["### Separate Source and Target pairs"]},{"cell_type":"code","metadata":{"id":"GTzjK4G6WGHb"},"source":["encoder_text = [] #Initialize Source language list\n","decoder_text = [] #Initialize Target language list\n","\n","#Iterate over data\n","for line in data:\n","    try:\n","        in_txt, out_txt,_ = line.split('\\t')\n","        encoder_text.append(in_txt)\n","        \n","        # Add tab '<start>' as 'start sequence in target\n","        # And '<end>' as End\n","        decoder_text.append('<start> ' + out_txt + ' <end>')\n","    except:\n","        pass #ignore data which goes into error        "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4T4HJ3A9WGHd"},"source":["### Separate Source and Target pairs.."]},{"cell_type":"code","metadata":{"id":"xyZqwGuvWGHe"},"source":["encoder_text[100:105]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v2CILOS3WGHg"},"source":["decoder_text[100:105]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CnRP431qWGHj"},"source":["### Tokenize Source language sentences"]},{"cell_type":"code","metadata":{"id":"9ucqBYr4WGHk"},"source":["#Tokenizer for source language\n","encoder_t = tf.keras.preprocessing.text.Tokenizer()\n","encoder_t.fit_on_texts(encoder_text) #Fit it on Source sentences\n","encoder_seq = encoder_t.texts_to_sequences(encoder_text) #Convert sentences to numbers \n","encoder_seq[100:105] #Display some converted sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zZv-TPVOWGHn"},"source":["#Maximum length of sentence\n","max_encoder_seq_length = max([len(txt) for txt in encoder_seq])\n","print('Maximum sentence length for Source language: ', max_encoder_seq_length)\n","\n","#Source language Vocablury\n","encoder_vocab_size = len(encoder_t.word_index)\n","print('Source language vocablury size: ', encoder_vocab_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BeASC50tWGHr"},"source":["### Tokenize Target language sentences"]},{"cell_type":"code","metadata":{"id":"Zcyow_keWGHr"},"source":["#Tokenizer for target language, filters should not <start> and <end>\n","#remove < and > used in Target language sequences\n","decoder_t = tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n","decoder_t.fit_on_texts(decoder_text) #Fit it on target sentences\n","decoder_seq = decoder_t.texts_to_sequences(decoder_text) #Convert sentences to numbers "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"phLrIK6WWGHt"},"source":["#Maximum length of sentence\n","max_decoder_seq_length = max([len(txt) for txt in decoder_seq])\n","print('Maximum sentence length for Target language: ', max_decoder_seq_length)\n","\n","#Target language Vocablury\n","decoder_vocab_size = len(decoder_t.word_index)\n","print('Target language vocablury size: ', decoder_vocab_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6fXf48uMWGH6"},"source":["### Compare different sentences length"]},{"cell_type":"code","metadata":{"id":"w0aRkCfiWGH7"},"source":["#Source Language sentences\n","print('Length for sentence number 100: ', len(encoder_seq[100]))\n","print('Length for sentence number 2000: ', len(encoder_seq[2000]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W2qZN4BvWGH_"},"source":["#Target Language sentences\n","print('Length for sentence number 100: ', len(decoder_seq[100]))\n","print('Length for sentence number 2000: ', len(decoder_seq[2000]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wmeSCSs6WGIC"},"source":["### How do we make it same?"]},{"cell_type":"markdown","metadata":{"id":"mqps8juMWGIE"},"source":["### Padding the sentences"]},{"cell_type":"code","metadata":{"id":"8rnbyRs9WGIF"},"source":["#Source sentences\n","encoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(encoder_seq, \n","                                                                   maxlen=max_encoder_seq_length, #22\n","                                                                   padding='pre')\n","\n","#Target Sentences\n","decoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(decoder_seq, \n","                                                                   maxlen=max_decoder_seq_length, #27\n","                                                                   padding='post')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a61g_ADpWGIH"},"source":["print('Source data shape: ', encoder_input_data.shape)\n","print('Target data shape: ', decoder_input_data.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nl4oxg8cWGIJ"},"source":["#### Integer to Word converter for Decoder data"]},{"cell_type":"code","metadata":{"id":"jhwnBD-0WGIK"},"source":["int_to_word_decoder = dict((i,c) for c, i in decoder_t.word_index.items())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q8TP07uxWGIP"},"source":["int_to_word_decoder[15]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5g4Y4oRvWGIV"},"source":["### Building Decoder Output"]},{"cell_type":"code","metadata":{"id":"YuoPA9aWWGIV"},"source":["import numpy as np\n","\n","#Initialize array\n","decoder_target_data = np.zeros((decoder_input_data.shape[0], decoder_input_data.shape[1]))\n","\n","#Shift Target output by one word\n","for i in range(decoder_input_data.shape[0]):\n","    for j in range(1,decoder_input_data.shape[1]):\n","        decoder_target_data[i][j-1] = decoder_input_data[i][j]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"haordQCuWGIX"},"source":["#### Convert target data in one hot vector"]},{"cell_type":"code","metadata":{"id":"Vs2SKKI5WGIY"},"source":["#Initialize one hot encoding array\n","decoder_target_one_hot = np.zeros((decoder_input_data.shape[0], #number of sentences\n","                                   decoder_input_data.shape[1], #Number of words in each sentence\n","                                   len(decoder_t.word_index)+1)) #Vocab size + 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xoX_6OP4WGIm"},"source":["#Build one hot encoded array\n","for i in range(decoder_target_data.shape[0]):\n","    for j in range(decoder_target_data.shape[1]):\n","        decoder_target_one_hot[i][j] = tf.keras.utils.to_categorical(decoder_target_data[i][j],\n","                                                                     num_classes=len(decoder_t.word_index)+1)    "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y5fO1TTWWGIo"},"source":["decoder_target_one_hot.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fQYFym0AWGIq"},"source":["### Building the Training Model"]},{"cell_type":"code","metadata":{"id":"XnDjHynRi24U"},"source":["tf.keras.backend.clear_session()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fRfYeB2AWGIr"},"source":["#Define config parameters\n","encoder_embedding_size = 50\n","decoder_embedding_size = 50\n","rnn_units = 128"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"71ukvjyeWGIt"},"source":["#### Build Encoder"]},{"cell_type":"code","metadata":{"id":"2SHW_YvkWGIw"},"source":["#Input Layer\n","encoder_inputs = tf.keras.layers.Input(shape=(22,))\n","\n","#Embedding layer\n","encoder_embedding = tf.keras.layers.Embedding(encoder_vocab_size+1, encoder_embedding_size)\n","\n","#Get embedding layer output by feeding inputs\n","encoder_embedding_output = encoder_embedding(encoder_inputs)\n","\n","#---Following code has been commented out for Attention-------\n","#LSTM Layer and its output\n","#x, state_h, state_c = tf.keras.layers.LSTM(rnn_units,return_state=True)(encoder_embedding_output)\n","\n","#Build a list to feed Decoder\n","#encoder_states = [state_h, state_c]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YbfLAFglWGIx"},"source":["#### Build Encoder - Get all hidden states"]},{"cell_type":"code","metadata":{"id":"EnZJnNSbWGIz"},"source":["#Create LSTM Layer and get All hidden states, last hidden and cell state\n","encoder_lstm = tf.keras.layers.LSTM(rnn_units,return_state=True, return_sequences=True)\n","\n","#Get 3 outputs of LSTM Layer\n","encoder_all_h_states, state_h, state_c = encoder_lstm(encoder_embedding_output)\n","\n","#Build a list to feed Decoder\n","encoder_states = [state_h, state_c]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1OcK7SYUjDdf"},"source":["encoder_all_h_states"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PBlBkAEGn29l"},"source":["state_h"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RAIuXWK5WGI1"},"source":["#### Build Decoder"]},{"cell_type":"code","metadata":{"id":"uYY-zwUSWGI2"},"source":["#Decode input - padded Target sentences\n","decoder_inputs = tf.keras.layers.Input(shape=(27,))\n","\n","#Decoder Embedding layer\n","decoder_embedding = tf.keras.layers.Embedding(decoder_vocab_size + 1, decoder_embedding_size)\n","\n","#Embedding layer output\n","decoder_embedding_output = decoder_embedding(decoder_inputs)\n","\n","#Decoder RNN\n","decoder_rnn = (tf.keras.layers.LSTM(rnn_units, return_sequences=True, return_state=True))\n","\n","#Decoder RNN Output, State initialization from Encoder states\n","#Output will be all hidden sequences, last 'h' state and last 'c' state\n","decoder_all_h_states, d_state_h, d_state_c = decoder_rnn(decoder_embedding_output,\n","                                                       initial_state=encoder_states)\n","\n","#---Following code has been commented out for Attention-------\n","#Output Layer\n","#decoder_dense = tf.keras.layers.Dense(decoder_vocab_size + 1, activation='softmax')\n","\n","#Output of Dense layer\n","#decoder_outputs = decoder_dense(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tLmAljrtjAGq"},"source":["decoder_all_h_states"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QwtwXesXWGI4"},"source":["#### Build Decoder...Attention Matrix"]},{"cell_type":"code","metadata":{"id":"mOJtf8kMWGI5"},"source":["#1. Dot Product between Decoder_all_h_states and encoder_all_h_states\n","#2. Apply softmax to get Attention matrix\n","\n","#Dimensions details\n","#decoder_all_states = batch_size x max_decoder_length x rnn_units\n","#encoder_all_states = batch_size x max_encoder_length x rnn_units\n","#score = batch_size x max_decoder_length x max_encoder_length\n","\n","score = tf.keras.layers.dot([decoder_all_h_states, encoder_all_h_states], axes=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c6WHyV7wpDse"},"source":["score"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pUq50cFvo5ST"},"source":["#Attention matrix = batch_size x max_decoder_length x max_encoder_length\n","attention_matrix = tf.keras.layers.Activation('softmax')(score)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SNzzFkm4W0N8"},"source":["attention_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3JzobuFK5mdL"},"source":["encoder_all_h_states"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cYFfIGyvWGI6"},"source":["#### Build Decoder...Context Vector"]},{"cell_type":"code","metadata":{"id":"h4ICneq6WGI6"},"source":["#Weighted sum of multiplication of attention_matrix and encoder states\n","#Dimension of context_vector =  batch_size x max_decoder_length x rnn_units\n","context_vector = tf.keras.layers.dot([attention_matrix, encoder_all_h_states], axes=[2,1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"S3EtN7Ho9fHy"},"source":["context_vector"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4GOKQuJiWGI8"},"source":["#### Build Decoder...Attention Vector"]},{"cell_type":"code","metadata":{"id":"L9aJaK5LWGI8"},"source":["#Concatenate context vector and decoder_all_h_states\n","#context_decoder_hidden = batch_size x max_decoder_length x rnn_units\n","#attention_vector = batch_size x max_decoder_length x 128\n","\n","context_decoder_hidden = tf.keras.layers.concatenate([context_vector, \n","                                                      decoder_all_h_states])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5MvkQb2ep3Y9"},"source":["context_decoder_hidden"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M9bWFe_tp2WZ"},"source":["attention_dense_layer = tf.keras.layers.Dense(128, activation='relu')\n","\n","attention_vector = attention_dense_layer(context_decoder_hidden)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v1nE-nHNfqix"},"source":["attention_vector"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yvcsEikBWGI-"},"source":["#### Build Decoder...Output layer"]},{"cell_type":"code","metadata":{"id":"GinU4PH5WGI_"},"source":["#Output layer\n","decoder_dense = tf.keras.layers.Dense(decoder_vocab_size + 1, activation='softmax')\n","\n","#With attention input will be attention_vector and not decoder_all_h_states\n","decoder_outputs = decoder_dense(attention_vector)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K2CHFVvmkB06"},"source":["decoder_outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"__FmqX3qWGJL"},"source":["### Build Model using both Encoder and Decoder"]},{"cell_type":"code","metadata":{"id":"IbtWE0_JWGJL"},"source":["model = tf.keras.models.Model([encoder_inputs, decoder_inputs], #2 Inputs to the model\n","                              decoder_outputs) #Output of the model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WG4pyK_jWGJO"},"source":["model.compile(optimizer='adam', \n","              loss='categorical_crossentropy', \n","              metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xA0TU2tCVYLO"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tf.keras.utils.tf.keras.utils.plot_model(model)"],"metadata":{"id":"acCTPB8T2oZt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uHhrDTBZWGJQ"},"source":["### Train the model"]},{"cell_type":"code","metadata":{"id":"81ciKSvCWGJQ"},"source":["model.fit([encoder_input_data, decoder_input_data], decoder_target_one_hot,\n","          batch_size=64,\n","          epochs=25,\n","          validation_split=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0gO-p-OaWGJT"},"source":["### Save the model for later reuse"]},{"cell_type":"code","metadata":{"id":"GUJcwtNyWGJU"},"source":["#model.save('models/seq2seq_training_translation_attention.hd5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PjuM6UkoWGJW"},"source":["model = tf.keras.models.load_model('models/seq2seq_training_translation_attention.hd5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z8k64d1HWGJY"},"source":["# Building Model for Prediction"]},{"cell_type":"markdown","metadata":{"id":"_w__FLXCWGJZ"},"source":["### Build the Encoder Model to predict Encoder States"]},{"cell_type":"code","metadata":{"id":"x9sSoCtMWGJZ"},"source":["encoder_model = tf.keras.models.Model(inputs=encoder_inputs, #Padded input sequences\n","                                      outputs=[encoder_all_h_states] + #Hidden states at all time steps\n","                                      encoder_states) #Hidden state and Cell state at last time step"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RScHHgr5WGJb"},"source":["### Build the Decoder Model \n","<p/>\n","\n","<ol><li>Define Input for both 'h' state and 'c' state initialization </li>\n","    <li><font color=\"blue\">Define Input for all encoder states - Attention Layer </font></li>\n","<li>Get Decoder RNN outputs along with h and c state</li>\n","<li><font color=\"blue\">Build Attention Layer</font></li>\n","<li><font color=\"blue\">Get Decoder Dense layer output using Attention vector</font></li>\n","    <li><font color=\"blue\">Build Model</font></li></ol>"]},{"cell_type":"markdown","metadata":{"id":"fZGRM_UQWGJc"},"source":["##### Step 1 - Define Input for both 'h' state and 'c' state initialization"]},{"cell_type":"code","metadata":{"id":"Wq4XvgTAWGJc"},"source":["#Hidden state input\n","decoder_state_input_h = tf.keras.layers.Input(shape=(rnn_units,))\n","\n","#Cell state input\n","decoder_state_input_c = tf.keras.layers.Input(shape=(rnn_units,))\n","\n","#Putting it together\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U4rg9wJMWGJe"},"source":["##### Step 2 - Define Input encoder states - Attention Layer"]},{"cell_type":"code","metadata":{"id":"SYozwBH2WGJe"},"source":["encoder_outputs = tf.keras.layers.Input(shape=(max_encoder_seq_length, rnn_units,))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cu7AJ7teWGJg"},"source":["##### Step 3 - Get Decoder RNN outputs along with h and c state"]},{"cell_type":"code","metadata":{"id":"TPbxVkdoWGJh"},"source":["#Get Embedding layer output\n","x = decoder_embedding(decoder_inputs)\n","\n","#We will use the layer which we trained earlier\n","rnn_outputs, state_h, state_c = decoder_rnn(x, initial_state=decoder_states_inputs)\n","\n","#Why do we need this?\n","decoder_states = [state_h, state_c]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YnDK-mtkWGJk"},"source":["##### Step 4 - Build Attention Layer"]},{"cell_type":"code","metadata":{"id":"9RFkceyHWGJk"},"source":["#Alignment score\n","p_score = tf.keras.layers.dot([rnn_outputs, encoder_outputs], axes=2)\n","\n","#Perform softmax to get Alignment matrix\n","p_alignment_matrix = tf.keras.layers.Activation('softmax')(p_score)\n","\n","#Context Vector\n","p_context_vector = tf.keras.layers.dot([p_alignment_matrix, encoder_outputs], axes=[2,1])\n","\n","#Build Attention Vector\n","# 1. Caoncatenate both context vector and decoder outputs\n","# 2. Feed it to the Dense layer \n","p_context_decoder_hidden = tf.keras.layers.concatenate([p_context_vector, rnn_outputs])\n","p_attention_vector = attention_dense_layer(p_context_decoder_hidden)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CLw9G3KkWGJl"},"source":["p_alignment_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fUVTE025WGJn"},"source":["##### Step 5 - Get Decoder Dense layer output"]},{"cell_type":"code","metadata":{"id":"o60fmWbJWGJo"},"source":["decoder_outputs = decoder_dense(p_attention_vector)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oE7o-lOfWGJ3"},"source":["##### Step 6 - Build Decoder Model"]},{"cell_type":"code","metadata":{"id":"545vI4cXWGJ4"},"source":["#3 Inputs - Word, h/c state and all hidden states from encoder\n","#3 Outputs - predicted word, h and c state values for next run and alignment matrix for visualization\n","\n","decoder_model = tf.keras.models.Model([decoder_inputs] +  #Start sequence and then word\n","                                      decoder_states_inputs + #h and c state value for initialization\n","                                      [encoder_outputs],  #Encoder all hidden states for Attention layer\n","                                      [decoder_outputs] + #Model word prediction\n","                                      decoder_states +   #h and c states for next run\n","                                      [p_alignment_matrix]) #for Alignment matrix"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"G7VEzNGyWGJ5"},"source":["# Predicting output from Seq2Seq model"]},{"cell_type":"markdown","metadata":{"id":"mcg5mBcYWGJ6"},"source":["##### Build a prediction function"]},{"cell_type":"code","metadata":{"id":"J1VoPsIkWGJ6"},"source":["def decode_sentence(input_sequence):\n","    \n","    #Get the encoder state values\n","    encoder_output =  encoder_model.predict(input_sequence)\n","    decoder_initial_states_value = encoder_output[1:]    \n","    encoded_seqs = encoder_output[0]\n","    \n","    #Build a sequence with '<start>' - starting sequence for Decoder\n","    target_seq = np.zeros((1,1))    \n","    target_seq[0][0] = decoder_t.word_index['<start>']\n","    \n","    #flag to check if prediction should be stopped\n","    stop_loop = False\n","    \n","    #Initialize predicted sentence\n","    predicted_sentence = ''\n","    \n","    #start the loop\n","    while not stop_loop:\n","        \n","        #Decoder model with 3 inputs\n","        predicted_outputs, h, c, a_matrix = decoder_model.predict([target_seq] + \n","                                                                  decoder_initial_states_value +\n","                                                                  [encoded_seqs])\n","        \n","        #Get the predicted word index with highest probability\n","        predicted_output = np.argmax(predicted_outputs[0,-1,:])\n","        \n","        #Get the predicted word from predicter index\n","        if (predicted_output == 0):\n","            predicted_word = ' '\n","        else:\n","            predicted_word = int_to_word_decoder[predicted_output]\n","        \n","        #Check if prediction should stop\n","        if(predicted_word == '<end>' or len(predicted_sentence) > max_decoder_seq_length):\n","            \n","            stop_loop = True\n","            continue\n","                    \n","        #Updated predicted sentence\n","        if (len(predicted_sentence) == 0):\n","            predicted_sentence = predicted_word\n","        else:\n","            predicted_sentence = predicted_sentence + ' ' + predicted_word\n","            \n","        #Update target_seq to be the predicted word index\n","        target_seq[0][0] = predicted_output\n","        \n","        #Update initial states value for decoder\n","        decoder_initial_states_value = [h,c]\n","        \n","        print(np.argmax(a_matrix[0][0]))\n","    \n","    return predicted_sentence"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CuQcVTVZWGJ8"},"source":["##### Call Prediction function on a random sentence"]},{"cell_type":"code","metadata":{"id":"jkJznTF8WGJ9"},"source":["#Generate a random number\n","start_num = np.random.randint(0, high=len(encoder_text) - 10)\n","\n","#Predict model output for 5 sentences\n","for i in range(start_num, start_num + 1):\n","    input_seq = encoder_input_data[i : i+1]\n","    #print(input_seq)\n","    predicted_sentence = decode_sentence(input_seq)\n","    print('--------')\n","    print ('Input sentence: ', encoder_text[i])\n","    print ('Predicted sentence: ', predicted_sentence )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YS-RjZPlWGJ-"},"source":["##### Save encoder and decoder model"]},{"cell_type":"code","metadata":{"id":"2l-qAT-bWGJ-"},"source":["#Compile models to avoid error\n","encoder_model.compile(optimizer='adam',loss='categorical_crossentropy')\n","decoder_model.compile(optimizer='adam',loss='categorical_crossentropy')\n","\n","#Save the models\n","encoder_model.save('models/seq2seq_encoder_eng_hin.hd5')  #Encoder model\n","decoder_model.save('models/seq2seq_decoder_eng_hin.hd5')  #Decoder model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Oh0okCv0WGKA"},"source":["##### Save encoder and decoder tokenizers"]},{"cell_type":"code","metadata":{"id":"Qwfb-v7OWGKB"},"source":["import pickle\n","\n","pickle.dump(encoder_t,open('models/encoder_tokenizer_eng','wb'))\n","pickle.dump(decoder_t,open('models/decoder_tokenizer_hin','wb'))"],"execution_count":null,"outputs":[]}]}