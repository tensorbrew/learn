{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"4b. Sentiment_Analysis_Google_Word2Vec_Embedding.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["mQKm7K78KMQa","40sSeDoWKMQx","2GgPOuSzKMRA","fKmVWM5pKMRF","m7CMlSVYCHNA","UTZqcuaqB2cl","wAOvV9C_KMRl","j_aH5TX5KMSA","2NvjDJo7OYOb"]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"mQKm7K78KMQa"},"source":["#### Load Movie reviews Dataset"]},{"cell_type":"markdown","metadata":{"id":"IS0axMJDKrph"},"source":["We will be using data available on Kaggle platform for this exercise. The data is available at https://www.kaggle.com/c/word2vec-nlp-tutorial/data."]},{"cell_type":"code","metadata":{"id":"WxnI1KLhLJ_J"},"source":["#Connect Google drive to colab\n","from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zDssadQzJnmz"},"source":["Load dataset"]},{"cell_type":"code","metadata":{"id":"1Y_ohNjGKt6R"},"source":["import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pb3r_KaeJJ0d"},"source":["#change file path to point to where you have stored the zip file.\n","df = pd.read_csv('/gdrive/My Drive/AI-ML/labeledTrainData.tsv.zip', header=0, delimiter=\"\\t\", quoting=3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l70lgDrNJZ-F"},"source":["print('Number of examples in Dataset: ', df.shape)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q12_oUDIIC-k"},"source":["df.loc[0, 'review']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"45uxRF4rKMQq"},"source":["Split Data into Training and Test Data"]},{"cell_type":"code","metadata":{"id":"cwNZ5XEpKMQq"},"source":["from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q-RNaPq2KMQt"},"source":["X_train, X_test, y_train, y_test = train_test_split(\n","    df['review'],\n","    df['sentiment'],\n","    test_size=0.2, \n","    random_state=42\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IdMWitcdByBs"},"source":["X_train.shape, X_test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"40sSeDoWKMQx"},"source":["#### Build the Tokenizer"]},{"cell_type":"code","metadata":{"id":"AGM55RRUM3fN"},"source":["import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LNB6T0EVKMQ6"},"source":["desired_vocab_size = 10000 #Vocablury size\n","t = tf.keras.preprocessing.text.Tokenizer(num_words=desired_vocab_size) # num_words -> Vocablury size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t65mfe_2KMQ8"},"source":["#Fit tokenizer with actual training data\n","t.fit_on_texts(X_train.tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6N7cgYEvVGzB"},"source":["#Vocabulary\n","t.word_index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I8wHv4GJV1lL"},"source":["len(t.word_index)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2GgPOuSzKMRA"},"source":["#### Prepare Training and Test Data"]},{"cell_type":"markdown","metadata":{"id":"8o8fG3FtKMRA"},"source":["Get the word index for each of the word in the review"]},{"cell_type":"code","metadata":{"id":"G9m65RFCVXCd"},"source":["X_train[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fNQIpYPKKMRB"},"source":["#Replace each word in the text with word's index\n","X_train = t.texts_to_sequences(X_train.tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xh1nDZFDVlB8"},"source":["print(X_train[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Gix3lNmKMRD"},"source":["X_test = t.texts_to_sequences(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hb9z28TZKMRF"},"source":["How many words in each review?"]},{"cell_type":"code","metadata":{"id":"O7maQ5kpxdfI"},"source":["len(X_train[2000])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fKmVWM5pKMRF"},"source":["#### Pad Sequences - Important"]},{"cell_type":"code","metadata":{"id":"h5YfEUx2KMRI"},"source":["#Define maximum number of words to consider in each review\n","max_review_length = 300"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aeJeFjogKMRM"},"source":["#Pad training and test reviews\n","X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train,\n","                                                        maxlen=max_review_length,\n","                                                        padding='post',\n","                                                        truncating='post')\n","X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, \n","                                                       maxlen=max_review_length, \n","                                                       padding='post',\n","                                                       truncating='post')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i4GLCWBOlztU"},"source":["X_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6QJqX-Z5wL-W"},"source":["X_test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HVteHr5IzS4I"},"source":["X_train[2000]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TqebOzGcE7O4"},"source":["#### Load Google Word2Vec model"]},{"cell_type":"markdown","metadata":{"id":"bjn2ttIgFbGS"},"source":["We can use gensim library to load pre-trained Word2Vec or Glove models. For list of available models can be found at [this url](https://github.com/RaRe-Technologies/gensim-data)."]},{"cell_type":"code","metadata":{"id":"wwOY4c75E_U0"},"source":["import gensim.downloader as api"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"z_A7HwsTFz4L"},"source":["#Load Google word2vec model\n","model = api.load('word2vec-google-news-300')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"org7teM_GR_j"},"source":["#Size of the model\n","model.vectors.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B-ENlh0jsxvi"},"source":["#Model's vocab\n","model.index2word"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LsgbyIoGIOpG"},"source":["#Embedding for word great\n","model['with']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UTZqcuaqB2cl"},"source":["#### Get Pre-trained Embeddings"]},{"cell_type":"code","metadata":{"id":"SbgVAzAPDl1O"},"source":["embedding_size = model.vector_size\r\n","embedding_size"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kBQOS906KX0p"},"source":["Google Word2Vec model has vocabulary size of 3M words. In this example, we have only 10000 words as vocabulary. This means we do not require entire Google Word2Vec model. Rather, we will only take the embeddings of the words that are in our dataset vocabulary."]},{"cell_type":"code","metadata":{"id":"HPNcZC9PEA8v"},"source":["#Initialize embedding matrix for our dataset with 10000+1 rows (1 for padding word)\n","#and 300 columns (as embedding size is 300)\n","embedding_matrix = np.zeros((desired_vocab_size + 1, embedding_size))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mufDrkM-EKlK"},"source":["#Load word vectors for each word from Google Word2Vec model\n","for word, i in sorted(t.word_index.items(),key=lambda x:x[1]):\n","    if i > (desired_vocab_size+1):\n","        break\n","    try:\n","        embedding_vector = model[word] #Reading word's embedding from Google Word2Vec\n","        embedding_matrix[i] = embedding_vector\n","    except:\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Ww5J3x_LGg3"},"source":["We now have word embeddings for our vocabulary words from Google Word2Vec model. We can now use it in our Model training."]},{"cell_type":"code","metadata":{"id":"qeZNhU2ZEs1w"},"source":["embedding_matrix.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wAOvV9C_KMRl"},"source":["#### Build Model - Dense Layers"]},{"cell_type":"code","metadata":{"id":"pWtUZzM3KMRs"},"source":["#Initialize model\n","tf.keras.backend.clear_session()\n","model = tf.keras.Sequential()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rbo6eeNGMtWn"},"source":["To handle, pre-trained embeddings, we will use Keras Embedding layer"]},{"cell_type":"code","metadata":{"id":"vUTG9uAMM-z3"},"source":["model.add(tf.keras.layers.Embedding(desired_vocab_size + 1, #Vocablury size\n","                                    embedding_size, #Embedding size\n","                                    weights=[embedding_matrix], #Embeddings taken from pre-trained model\n","                                    trainable=False, #As embeddings are already available, we will not train this layer. It will act as lookup layer.\n","                                    input_length=max_review_length) #Number of words in each review\n","          )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KKyc5UQSMDQG"},"source":["Embedding Layer gives us 3D output ->\n","[Batch_Size , Review Length , Embedding_Size]"]},{"cell_type":"code","metadata":{"id":"ezX7QcD8NSmw"},"source":["model.output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iWcKzPbsMFZx"},"source":["#Flatten the data as we will use Dense layers\n","model.add(tf.keras.layers.Flatten())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NAActtdbeeCH"},"source":["model.output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DF_wJp6sKMRv"},"source":["Add Hidden layers"]},{"cell_type":"code","metadata":{"id":"iignS5XqKMRv"},"source":["#Add Hidden layers (Dense layers)\n","model.add(tf.keras.layers.Dense(100, activation='relu'))\n","model.add(tf.keras.layers.BatchNormalization())\n","model.add(tf.keras.layers.Dense(50, activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.5))\n","model.add(tf.keras.layers.Dense(25, activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.5))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8ua5Dj8LVk2"},"source":["Add Output layer"]},{"cell_type":"code","metadata":{"id":"9esFq2mNZfFZ"},"source":["model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GobXBLHXKMR9"},"source":["#Compile the model\n","model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iueK1G3pOznW"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j_aH5TX5KMSA"},"source":["##### Train Model"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"AV3TceqjKMSC"},"source":["model.fit(X_train,y_train,\n","          epochs=5,\n","          batch_size=32,          \n","          validation_data=(X_test, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2NvjDJo7OYOb"},"source":["#### Building a CNN Model"]},{"cell_type":"markdown","metadata":{"id":"PlhUrG_EO2Ga"},"source":["Start a model"]},{"cell_type":"code","metadata":{"id":"tc0ZANd3OaWs"},"source":["model2 = tf.keras.Sequential()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eRX6YC7_O3to"},"source":["Add Embedding layer to handle Word2Vec"]},{"cell_type":"code","metadata":{"id":"gF2fufQFOx2w"},"source":["model2.add(tf.keras.layers.Embedding(desired_vocab_size + 1, #Vocablury size\n","                                    embedding_size, #Embedding size\n","                                    weights=[embedding_matrix], #Embeddings taken from pre-trained model\n","                                    trainable=False, #As embeddings are already available, we will not train this layer. It will act as lookup layer.\n","                                    input_length=max_review_length) #Number of words in each review\n","          )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J4N0K2fczXG8"},"source":["model2.output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VJRM3ICZO8lD"},"source":["Add Conv1D hidden layers : As our text data is 2D (number of words, Embedding size), we will use Conv1D in this case (compared to Conv2D with images which are 3D)"]},{"cell_type":"code","metadata":{"id":"3a03h-_9OmqL"},"source":["#Add first convolutional layer\n","model2.add(tf.keras.layers.Conv1D(32, #Number of filters \n","                                 kernel_size=(3), #Size of the filter\n","                                 strides=1,\n","                                 activation='relu'))\n","\n","#normalize data\n","model2.add(tf.keras.layers.BatchNormalization())\n","\n","#Add second convolutional layer\n","model2.add(tf.keras.layers.Conv1D(64, kernel_size=(3), strides=2))\n","model2.add(tf.keras.layers.ReLU())\n","\n","#normalize data\n","model2.add(tf.keras.layers.BatchNormalization())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fKJbqsCNPTLr"},"source":["#Use Global Average Pooling\n","model2.add(tf.keras.layers.GlobalAveragePooling1D())\n","\n","#Output layer\n","model2.add(tf.keras.layers.Dense(1, activation='sigmoid'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FZCe3ghEPjyx"},"source":["#Compile the model\n","model2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ekq97ri0Pne2"},"source":["model2.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0w0oGtKpPxkp"},"source":["model2.fit(X_train,y_train,\n","          epochs=5,\n","          batch_size=32,          \n","          validation_data=(X_test, y_test))"],"execution_count":null,"outputs":[]}]}