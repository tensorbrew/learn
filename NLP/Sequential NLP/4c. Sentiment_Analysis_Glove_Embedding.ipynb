{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"},"colab":{"name":"4c. Sentiment_Analysis_Glove_Embedding.ipynb","private_outputs":true,"provenance":[],"collapsed_sections":["fKmVWM5pKMRF","wAOvV9C_KMRl","j_aH5TX5KMSA","2NvjDJo7OYOb"]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"mQKm7K78KMQa"},"source":["#### Load Movie reviews Dataset"]},{"cell_type":"markdown","metadata":{"id":"IS0axMJDKrph"},"source":["We will be using data available on Kaggle platform for this exercise. The data is available at https://www.kaggle.com/c/word2vec-nlp-tutorial/data."]},{"cell_type":"code","metadata":{"id":"WxnI1KLhLJ_J"},"source":["#Connect Google drive to colab\n","from google.colab import drive\n","drive.mount('/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zDssadQzJnmz"},"source":["Load dataset"]},{"cell_type":"code","metadata":{"id":"1Y_ohNjGKt6R"},"source":["import pandas as pd\n","import numpy as np"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pb3r_KaeJJ0d"},"source":["#change file path to point to where you have stored the zip file.\n","df = pd.read_csv('/gdrive/My Drive/AI-ML/labeledTrainData.tsv.zip', header=0, delimiter=\"\\t\", quoting=3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l70lgDrNJZ-F"},"source":["print('Number of examples in Dataset: ', df.shape)\n","df.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q12_oUDIIC-k"},"source":["df.loc[0, 'review']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"45uxRF4rKMQq"},"source":["Split Data into Training and Test Data"]},{"cell_type":"code","metadata":{"id":"cwNZ5XEpKMQq"},"source":["from sklearn.model_selection import train_test_split"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q-RNaPq2KMQt"},"source":["X_train, X_test, y_train, y_test = train_test_split(\n","    df['review'],\n","    df['sentiment'],\n","    test_size=0.2, \n","    random_state=42\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IdMWitcdByBs"},"source":["X_train.shape, X_test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l601593pEBwR"},"source":["X_train.reset_index(inplace=True, drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f7-Z52AFegRh"},"source":["X_test.reset_index(inplace=True, drop=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ucd13PwUDX9g"},"source":["X_train"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"40sSeDoWKMQx"},"source":["#### Build the Tokenizer"]},{"cell_type":"code","metadata":{"id":"AGM55RRUM3fN"},"source":["import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LNB6T0EVKMQ6"},"source":["desired_vocab_size = 10000 #Vocablury size\n","t = tf.keras.preprocessing.text.Tokenizer(num_words=desired_vocab_size) # num_words -> Vocablury size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"t65mfe_2KMQ8"},"source":["#Fit tokenizer with actual training data\n","t.fit_on_texts(X_train.tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6N7cgYEvVGzB"},"source":["#Vocabulary\n","print(t.word_index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cfd1NYP7sG5e"},"source":["len(t.word_index)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2GgPOuSzKMRA"},"source":["#### Prepare Training and Test Data"]},{"cell_type":"markdown","metadata":{"id":"8o8fG3FtKMRA"},"source":["Get the word index for each of the word in the review"]},{"cell_type":"code","metadata":{"id":"G9m65RFCVXCd"},"source":["X_train[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fNQIpYPKKMRB"},"source":["X_train = t.texts_to_sequences(X_train.tolist())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xh1nDZFDVlB8"},"source":["print(X_train[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Gix3lNmKMRD"},"source":["X_test = t.texts_to_sequences(X_test)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hb9z28TZKMRF"},"source":["How many words in each review?"]},{"cell_type":"code","metadata":{"id":"O7maQ5kpxdfI"},"source":["len(X_train[200])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fKmVWM5pKMRF"},"source":["#### Pad Sequences - Important"]},{"cell_type":"code","metadata":{"id":"h5YfEUx2KMRI"},"source":["#Define maximum number of words to consider in each review\n","max_review_length = 300"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aeJeFjogKMRM"},"source":["#Pad training and test reviews\n","X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train,\n","                                                        maxlen=max_review_length,\n","                                                        padding='pre',\n","                                                        truncating='post')\n","X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, \n","                                                       maxlen=max_review_length, \n","                                                       padding='pre',truncating='post')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i4GLCWBOlztU"},"source":["X_train.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6QJqX-Z5wL-W"},"source":["X_test.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HVteHr5IzS4I"},"source":["X_train[200]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m7CMlSVYCHNA"},"source":["#### Load Glove model\n","\n","We can use gensim library to load pre-trained Word2Vec or Glove models. For list of available models can be found at [this url](https://github.com/RaRe-Technologies/gensim-data)."]},{"cell_type":"code","metadata":{"id":"C6cDJb-RR0nn"},"source":["import gensim.downloader as api"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3DNgLyLgR3H_"},"source":["#Load Glove model (similar to Word2Vec)\n","glove_model = api.load('glove-wiki-gigaword-50')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lewBKMuRR5W5"},"source":["#Model vocabulary\n","#glove_model.index2word"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"K9jb8OpbJR91"},"source":["#Size of the model\n","glove_model.vectors.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tp64MQ8WJYL0"},"source":["#Embedding for word great\n","glove_model['and']"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UTZqcuaqB2cl"},"source":["#### Get Pre-trained Embeddings"]},{"cell_type":"markdown","metadata":{"id":"n3Ftgb1zSDvG"},"source":["Pre-trained Glove model has 400,000 unique words (Vocabulary size). We do not need all the words. Moreover, we have to arrange word embeddings according to word index created by our tokenizers above. So we will extract word embeddings for only the words that we are interested in."]},{"cell_type":"code","metadata":{"id":"SbgVAzAPDl1O"},"source":["#Embedding length based on selected model - we are using 50d here.\n","embedding_vector_length = glove_model.vector_size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YnNv2qROvFur"},"source":["embedding_vector_length"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kBQOS906KX0p"},"source":["Initialize a embedding matrix which we will populate for our vocabulary words."]},{"cell_type":"code","metadata":{"id":"HPNcZC9PEA8v"},"source":["#Initialize embedding matrix for our dataset with 10000+1 rows (1 for padding word)\n","#and 50 columns (as embedding size is 50)\n","embedding_matrix = np.zeros((desired_vocab_size + 1, embedding_vector_length))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QyFHzDS05ouW"},"source":["embedding_matrix.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SVaLTX2cTZ79"},"source":["Load word vectors for each word in our vocabulary from from Glove pre-trained model"]},{"cell_type":"code","metadata":{"id":"mufDrkM-EKlK"},"source":["for word, i in sorted(t.word_index.items(),key=lambda x:x[1]):\n","    if i > (desired_vocab_size+1):\n","        break\n","    try:\n","        embedding_vector = glove_model[word] #Reading word's embedding from Glove model for a given word\n","        embedding_matrix[i] = embedding_vector\n","    except:\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Ww5J3x_LGg3"},"source":["We now have word embeddings for our vocabulary words from Glove model. We can now use it in our Model training."]},{"cell_type":"code","metadata":{"id":"qeZNhU2ZEs1w"},"source":["#Embedding for word 'movie' - index 17\n","embedding_matrix[17]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wAOvV9C_KMRl"},"source":["#### Build Model - Dense Layers"]},{"cell_type":"code","metadata":{"id":"pWtUZzM3KMRs"},"source":["#Initialize model\n","tf.keras.backend.clear_session()\n","model = tf.keras.Sequential()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Rbo6eeNGMtWn"},"source":["To handle, pre-trained embeddings, we will use Keras Embedding layer"]},{"cell_type":"code","metadata":{"id":"vUTG9uAMM-z3"},"source":["model.add(tf.keras.layers.Embedding(desired_vocab_size + 1, #Vocablury size\n","                                    embedding_vector_length, #Embedding size\n","                                    weights=[embedding_matrix], #Embeddings taken from pre-trained model\n","                                    trainable=False, #As embeddings are already available, we will not train this layer. It will act as lookup layer.\n","                                    input_length=max_review_length) #Number of words in each review\n","          )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KKyc5UQSMDQG"},"source":["Embedding Layer gives us 3D output ->\n","[Batch_Size , Review Length , Embedding_Size]"]},{"cell_type":"code","metadata":{"id":"ezX7QcD8NSmw"},"source":["model.output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DF_wJp6sKMRv"},"source":["Add Hidden layers"]},{"cell_type":"code","metadata":{"id":"u6EM2vbwyhTs"},"source":["#Flatten the data as we will use Dense layers\n","model.add(tf.keras.layers.Flatten())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pNy0uIerykGd"},"source":["model.output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iignS5XqKMRv"},"source":["#Add Hidden layers (Dense layers)\n","model.add(tf.keras.layers.BatchNormalization())\n","model.add(tf.keras.layers.Dense(100, activation='relu', input_shape=()))\n","model.add(tf.keras.layers.BatchNormalization())\n","model.add(tf.keras.layers.Dense(50, activation='relu'))\n","model.add(tf.keras.layers.BatchNormalization())\n","model.add(tf.keras.layers.Dense(25, activation='relu'))\n","model.add(tf.keras.layers.Dropout(0.25))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Q8ua5Dj8LVk2"},"source":["Add Output layer"]},{"cell_type":"code","metadata":{"id":"9esFq2mNZfFZ"},"source":["model.add(tf.keras.layers.Dense(1, activation='sigmoid'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GobXBLHXKMR9"},"source":["#Compile the model\n","model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iueK1G3pOznW"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"j_aH5TX5KMSA"},"source":["##### Train Model"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"AV3TceqjKMSC"},"source":["model.fit(X_train,y_train,\n","          epochs=5,\n","          batch_size=32,          \n","          validation_data=(X_test, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2NvjDJo7OYOb"},"source":["#### Building a CNN Model"]},{"cell_type":"markdown","metadata":{"id":"PlhUrG_EO2Ga"},"source":["Start a model"]},{"cell_type":"code","metadata":{"id":"tc0ZANd3OaWs"},"source":["tf.keras.backend.clear_session()\n","model2 = tf.keras.Sequential()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eRX6YC7_O3to"},"source":["Add Embedding layer to handle Word2Vec"]},{"cell_type":"code","metadata":{"id":"gF2fufQFOx2w"},"source":["model2.add(tf.keras.layers.Embedding(desired_vocab_size + 1, #Vocablury size\n","                                    embedding_vector_length, #Embedding size\n","                                    weights=[embedding_matrix], #Embeddings taken from pre-trained model\n","                                    trainable=False, #As embeddings are already available, we will not train this layer. It will act as lookup layer.\n","                                    input_length=max_review_length) #Number of words in each review\n","          )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aQ8iJvs-0dw0"},"source":["model2.output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VJRM3ICZO8lD"},"source":["Add Conv1D hidden layers : As our text data is 2D (number of words, Embedding size), we will use Conv1D in this case (compared to Conv2D with images which are 3D)"]},{"cell_type":"code","metadata":{"id":"3a03h-_9OmqL"},"source":["#Add first convolutional layer\n","model2.add(tf.keras.layers.Conv1D(32, #Number of filters \n","                                 kernel_size=(3), #Size of the filter\n","                                 strides=1,\n","                                 activation='relu'))\n","\n","#normalize data\n","model2.add(tf.keras.layers.BatchNormalization())\n","\n","#Add second convolutional layer\n","model2.add(tf.keras.layers.Conv1D(64, kernel_size=(3), strides=2))\n","model2.add(tf.keras.layers.ReLU())\n","\n","#normalize data\n","model2.add(tf.keras.layers.BatchNormalization())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T_saV2sp1ktz"},"source":["model2.output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fKJbqsCNPTLr"},"source":["#Use Global Average Pooling\n","model2.add(tf.keras.layers.GlobalAveragePooling1D())\n","\n","model2.output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1fWWQhJq152W"},"source":["#Output layer\n","model2.add(tf.keras.layers.Dense(1, activation='sigmoid'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FZCe3ghEPjyx"},"source":["#Compile the model\n","model2.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ekq97ri0Pne2"},"source":["model2.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0w0oGtKpPxkp"},"source":["model2.fit(X_train,y_train,\n","          epochs=5,\n","          batch_size=32,          \n","          validation_data=(X_test, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yQgjw4YM_r3o"},"source":["model2.fit(X_train,y_train,\n","          epochs=5,\n","          batch_size=32,          \n","          validation_data=(X_test, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WjoOSzi0B1JP"},"source":["11000/1500000"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zj4gFLGxEAX7"},"source":["### What if we want to train our own embedding"]},{"cell_type":"code","metadata":{"id":"BnomaF8nEJ4W"},"source":["tf.keras.backend.clear_session()\n","model3 = tf.keras.Sequential()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UUPlzkXsEN7A"},"source":["model3.add(tf.keras.layers.Embedding(desired_vocab_size + 1, #Vocablury size\n","                                    embedding_vector_length, #Embedding size\n","                                    #weights=[embedding_matrix], #Embeddings taken from pre-trained model\n","                                    #trainable=False, #As embeddings are already available, we will not train this layer. It will act as lookup layer.\n","                                    input_length=max_review_length) #Number of words in each review\n","          )"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J3pCuP6jEU5V"},"source":["#Add first convolutional layer\n","model3.add(tf.keras.layers.Conv1D(32, #Number of filters \n","                                 kernel_size=(3), #Size of the filter\n","                                 strides=1,\n","                                 activation='relu'))\n","\n","#normalize data\n","model3.add(tf.keras.layers.BatchNormalization())\n","\n","#Add second convolutional layer\n","model3.add(tf.keras.layers.Conv1D(64, kernel_size=(3), strides=2))\n","model3.add(tf.keras.layers.ReLU())\n","\n","#normalize data\n","model3.add(tf.keras.layers.BatchNormalization())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HiCC8Pq7Eefh"},"source":["#Use Global Average Pooling\n","model3.add(tf.keras.layers.GlobalAveragePooling1D())\n","\n","model3.output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VGgGZaugEj4f"},"source":["#Output layer\n","model3.add(tf.keras.layers.Dense(1, activation='sigmoid'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5YyVGyGVE4f6"},"source":["#Compile the model\n","model3.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hThtobPcAP8M"},"source":["model3.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LE1o3Of9EuXk"},"source":["model3.fit(X_train,y_train,\n","          epochs=5,\n","          batch_size=32,          \n","          validation_data=(X_test, y_test))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"h9Ki4PYUB5k7"},"source":["model3.save('movie_review_cnn.h5')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zAzFg_mYCBmC"},"source":["model3.predict(X_test[0:1])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TByl1DLACMa-"},"source":["y_test[0:1]"],"execution_count":null,"outputs":[]}]}