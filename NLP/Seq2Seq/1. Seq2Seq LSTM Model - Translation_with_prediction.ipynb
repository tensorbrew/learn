{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":["r-IatvBFKXnY","iDZlu731KXob"]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"5csO-wPPKXmw"},"source":["### Load tensorflow"]},{"cell_type":"code","metadata":{"id":"W0ttJZT3KXmx"},"source":["import tensorflow as tf"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"80Pcvzu4KXmz"},"source":["### Read the data\n","<font size=\"2\">Data for this exercise can be downloaded from http://www.manythings.org/anki/</font>"]},{"cell_type":"code","metadata":{"id":"vQBFJRquKXm0"},"source":["#You can use wget to download the file directly\n","!wget https://www.manythings.org/anki/hin-eng.zip --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E4V6ZdyClsfE"},"source":["!ls -l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JtMmO7f0aJdg"},"source":["#Unzip\n","!unzip hin-eng.zip"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QuTZABCz9qJC"},"source":["!ls -l"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jPUrsgVkaPi7"},"source":["!cat hin.txt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tooO9pGrKXm2"},"source":["import zipfile\n","import io\n","\n","#Read the zip file\n","zf = zipfile.ZipFile('hin-eng.zip', 'r')\n","\n","#Extract data from zip file\n","data = ''\n","with zf.open('hin.txt') as readfile:\n","  for line in io.TextIOWrapper(readfile, 'utf-8'):\n","    data += line"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fiQ2ROWgKXm3"},"source":["len(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["type(data)"],"metadata":{"id":"4s9uwLRs48Di"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Ir1xfYwKXm5"},"source":["print(data[40000:50000])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A6AgsSZ9KXm7"},"source":["\n","### Extract Source and Target Language pairs"]},{"cell_type":"code","metadata":{"id":"T1Mb8tIyKXm7"},"source":["#Split by newline character\n","data =  data.split('\\n')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"apuKzGY4KXm9"},"source":["len(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CzP2DaIUs0uw"},"source":["#Show some Data\n","data[100:105]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xnEw_IMEKXm_"},"source":["### Separate Source and Target pairs"]},{"cell_type":"code","metadata":{"id":"yQMiDL45KXnA"},"source":["encoder_text = [] #Initialize Source language list\n","decoder_text = [] #Initialize Target language list\n","\n","#Iterate over data\n","for line in data:\n","    try:\n","        in_txt, out_txt, attributiontxttxt = line.split('\\t')\n","        encoder_text.append(in_txt)\n","\n","        # Add tab '<start>' as 'start sequence in target\n","        # And '<end>' as End\n","        decoder_text.append('<start> ' + out_txt + ' <end>')\n","    except:\n","        pass #ignore data which goes into error"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JdOMuGbUKXnC"},"source":["### Separate Source and Target pairs.."]},{"cell_type":"code","metadata":{"id":"7K8QDZAbKXnD"},"source":["#English (Source)\n","encoder_text[100:105]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0DduIsYLKXnF"},"source":["#Target language\n","decoder_text[100:105]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9S_gOPOndgOy"},"source":["len(encoder_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fty0Xlc9hErF"},"source":["len(decoder_text)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1BA2RbOVKXnI"},"source":["### Tokenize Source language sentences"]},{"cell_type":"code","metadata":{"id":"2OiGqsXDKXnI"},"source":["#Tokenizer for source language\n","encoder_t = tf.keras.preprocessing.text.Tokenizer(lower=True)\n","encoder_t.fit_on_texts(encoder_text) #Fit it on Source sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"38f6Yl9HKXnK"},"source":["#Vocab\n","len(encoder_t.word_index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u8fZ-qAJ96Sw"},"source":["print(encoder_t.word_index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UgM11oD9b1kI"},"source":["#Convert English text to indexes\n","encoder_seq = encoder_t.texts_to_sequences(encoder_text) #Convert sentences to numbers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cN5DneFYp0nc"},"source":["encoder_text[100:105]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3c9RPZHOm7FR"},"source":["encoder_seq[100:105] #Display some converted sentences"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rqDABzT3VUXg"},"source":["#Maximum length of sentence\n","max_encoder_seq_length = max([len(txt) for txt in encoder_seq])\n","print('Maximum sentence length for Source language: ', max_encoder_seq_length)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kyFL1wvpKXnL"},"source":["#Source language Vocablury\n","encoder_vocab_size = len(encoder_t.word_index)\n","print('Source language vocablury size: ', encoder_vocab_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fOmkUnoLKXnN"},"source":["### Tokenize Target language sentences"]},{"cell_type":"code","metadata":{"id":"KLV2B3TXKXnO"},"source":["#Tokenizer for target language, filters should not <start> and <end>\n","#remove < and > used in Target language sequences\n","#decoder_t = tf.keras.preprocessing.text.Tokenizer()\n","decoder_t = tf.keras.preprocessing.text.Tokenizer(filters='!\"#$%&()*+,-./:;=?@[\\\\]^_`{|}~\\t\\n')\n","\n","decoder_t.fit_on_texts(decoder_text) #Fit it on target sentences\n","\n","decoder_seq = decoder_t.texts_to_sequences(decoder_text) #Convert sentences to numbers"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BfvJSLT9zLfK"},"source":["print(decoder_t.word_index)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(decoder_t.word_index)"],"metadata":{"id":"8XcqTcaC7cie"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7PghlcdYzD2o"},"source":["decoder_text[100:105]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yIBPAxxuzH-C"},"source":["decoder_seq[100:105]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n9M_6R01KXnP"},"source":["#Maximum length of sentence\n","max_decoder_seq_length = max([len(txt) for txt in decoder_seq])\n","print('Maximum sentence length for Target language: ', max_decoder_seq_length)\n","\n","#Target language Vocablury\n","decoder_vocab_size = len(decoder_t.word_index)\n","print('Target language vocablury size: ', decoder_vocab_size)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hi_fC8V3KXnR"},"source":["### Compare different sentences length"]},{"cell_type":"code","metadata":{"id":"XaoE7nULKXnS"},"source":["#Source Language sentences\n","print('Length for sentence number 100: ', len(encoder_seq[1000]))\n","print('Length for sentence number 150: ', len(encoder_seq[1500]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZFRd3s8XKXnV"},"source":["#Target Language sentences\n","print('Length for sentence number 100: ', len(decoder_seq[100]))\n","print('Length for sentence number 150: ', len(decoder_seq[1500]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r-IatvBFKXnY"},"source":["### How do we make it same?"]},{"cell_type":"markdown","metadata":{"id":"WNZ8AeLKKXnZ"},"source":["### Padding the sentences"]},{"cell_type":"code","metadata":{"id":"ZfKDl8IAKXna"},"source":["#Source sentences\n","encoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(encoder_seq,\n","                                                                   maxlen=max_encoder_seq_length,\n","                                                                   padding='pre')\n","\n","#Target Sentences\n","decoder_input_data = tf.keras.preprocessing.sequence.pad_sequences(decoder_seq,\n","                                                                   maxlen=max_decoder_seq_length,\n","                                                                   padding='post')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZLTwqyLyKXnc"},"source":["print('Source data shape: ', encoder_input_data.shape)\n","print('Target data shape: ', decoder_input_data.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LHIXCliIKXne"},"source":["encoder_text[100]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TgBriKXSKXnf"},"source":["encoder_input_data[100]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vJMPcCflKXnh"},"source":["decoder_text[100]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QokbLugoKXnj"},"source":["decoder_input_data[100]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"vZoH9-bJKXnk"},"source":["#### Integer to Word converter for Decoder data"]},{"cell_type":"code","metadata":{"id":"IuoP4YMeKXnl"},"source":["int_to_word_decoder = dict((i,c) for c, i in decoder_t.word_index.items())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yVOpq75gKXnm"},"source":["print(int_to_word_decoder)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aPTsEs-DKXno"},"source":["### Building Decoder Output"]},{"cell_type":"code","metadata":{"id":"T0Edu4F4KXnp"},"source":["decoder_input_data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rtrzQgsDKXns"},"source":["import numpy as np\n","\n","#Initialize array\n","decoder_target_data = np.zeros((decoder_input_data.shape[0],\n","                                decoder_input_data.shape[1]))\n","\n","#Shift Target output by one word\n","for i in range(decoder_input_data.shape[0]):\n","    for j in range(1,decoder_input_data.shape[1]):\n","        decoder_target_data[i][j-1] = decoder_input_data[i][j]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sVjADS08KXnt"},"source":["#decoder_t.word_index"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m5V8wz3XKXnv"},"source":["#<start> yeh kitab hai <end>\n","#Yeh kitab hai <end> 0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q1r_VVX0j2lU"},"source":["decoder_text[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gL9a-mGTKXn2"},"source":["decoder_input_data[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R-norMzYKXn5"},"source":["decoder_target_data[0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"enBdwD6PLOVE"},"source":["decoder_target_data.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jHH_em9TKXn6"},"source":["#### Convert target data in one hot vector"]},{"cell_type":"code","metadata":{"id":"DUXNoSeHKXn7"},"source":["#Initialize one hot encoding array\n","decoder_target_one_hot = np.zeros((decoder_input_data.shape[0], #number of sentences\n","                                   decoder_input_data.shape[1], #Number of words in each sentence\n","                                   len(decoder_t.word_index)+1)) #Vocab size + 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YvDD6DZrKXn9"},"source":["decoder_target_one_hot.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N5h-PeIGKXoA"},"source":["#Build one hot encoded array\n","for i in range(decoder_target_data.shape[0]):\n","    for j in range(decoder_target_data.shape[1]):\n","        decoder_target_one_hot[i][j] = tf.keras.utils.to_categorical(decoder_target_data[i][j],\n","                                                                     num_classes=len(decoder_t.word_index)+1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5Emzjo90KXoB"},"source":["decoder_target_one_hot.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eOAOZrxBgK5c"},"source":["#1st example\n","print(decoder_target_one_hot[0][2])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q6BNPWZ_KXoD"},"source":["### Building the Training Model"]},{"cell_type":"code","metadata":{"id":"q05uAVLXKXoD"},"source":["#Define config parameters\n","encoder_embedding_size = 50\n","decoder_embedding_size = 50\n","memory_size = 128 #Memory size for LSTM"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9M9eJGmSKXoF"},"source":["#### Build Encoder"]},{"cell_type":"code","metadata":{"id":"iJIqwk7aKkJ9"},"source":["tf.keras.backend.clear_session()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x5P1hzBqKXoG"},"source":["#Input Layer\n","encoder_inputs = tf.keras.layers.Input(shape=(22,))\n","\n","#Embedding layer - Word2Vec\n","encoder_embedding = tf.keras.layers.Embedding(encoder_vocab_size+1, #Size for One hot encoding\n","                                              encoder_embedding_size) #How many numbers to use for each word\n","#Get embedding layer output by feeding inputs\n","encoder_embedding_output = encoder_embedding(encoder_inputs)\n","\n","#LSTM Layer and its output\n","x, state_h, state_c = tf.keras.layers.LSTM(memory_size,\n","                                           return_state=True,\n","                                           dropout=0.2,\n","                                           recurrent_dropout=0.3)(encoder_embedding_output)\n","\n","#Build a list to feed Decoder - Sentence Embedding\n","encoder_states = [state_h, state_c]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eimPCBlvxUTH"},"source":["encoder_embedding_output"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["state_h"],"metadata":{"id":"Q7HyD5g9H6XE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["state_c"],"metadata":{"id":"VqJoehK7H8gF"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jrcqxcfhKXoH"},"source":["encoder_states"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EJvG6HRdKXoM"},"source":["#### Build Decoder"]},{"cell_type":"code","metadata":{"id":"3LQEt7zPKXoM"},"source":["#Decode input - padded Target sentences\n","decoder_inputs = tf.keras.layers.Input(shape=(27,))\n","\n","#Decoder Embedding layer\n","decoder_embedding = tf.keras.layers.Embedding(decoder_vocab_size + 1,\n","                                              decoder_embedding_size)\n","\n","#Embedding layer output\n","decoder_embedding_output = decoder_embedding(decoder_inputs)\n","\n","#Decoder RNN\n","decoder_rnn = tf.keras.layers.LSTM(memory_size,\n","                                   dropout=0.2,\n","                                   recurrent_dropout=0.3,\n","                                   return_sequences=True,\n","                                   return_state=True)\n","\n","#Decoder RNN Output, State initialization from Encoder states\n","#Output will be all hidden sequences, last 'h' state and last 'c' state\n","all_hidden_states_d,last_hidden_state_d,last_cell_state_decoder = decoder_rnn(decoder_embedding_output,\n","                                                                            initial_state=encoder_states)\n","\n","#Output Layer\n","decoder_dense = tf.keras.layers.Dense(decoder_vocab_size + 1, #+1 to make sure one-hot encoding works for highest index value\n","                                      activation='softmax')\n","\n","#Output of Dense layer\n","decoder_outputs = decoder_dense(all_hidden_states_d)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_MP3Xbx6v3qT"},"source":["#All hidden states\n","all_hidden_states_d"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"byZkwGJNKXoR"},"source":["#Predicted probabilities\n","decoder_outputs"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6kabGUQ4KXoT"},"source":["### Build Model using both Encoder and Decoder"]},{"cell_type":"code","metadata":{"id":"RkJyiM7uKXoT"},"source":["#Build a Seq2Seq model -> Encoder + Decoder\n","model = tf.keras.models.Model([encoder_inputs, decoder_inputs], #2 Inputs to the model\n","                              decoder_outputs) #Output of the model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"x-Yfmp3AKXoY"},"source":["model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mQ4npEHA3xd4"},"source":["model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I30VxT7LKXoa"},"source":["### Train the model"]},{"cell_type":"code","metadata":{"id":"W-uw8R3_KXoa"},"source":["model.fit([encoder_input_data, decoder_input_data],\n","          decoder_target_one_hot,\n","          batch_size=64,\n","          epochs=10,\n","          validation_split=0.2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iDZlu731KXob"},"source":["### Save the model for later reuse"]},{"cell_type":"code","metadata":{"id":"jpUOjtksKXoc"},"source":["model.save('seq2seq_training_translation.hd5')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"guuaYKaDKXod"},"source":["# Building Model for Prediction"]},{"cell_type":"markdown","metadata":{"id":"j-tXkDaqKXod"},"source":["### Build the Encoder Model to predict Encoder States"]},{"cell_type":"code","metadata":{"id":"QdM2QenRKXod"},"source":["encoder_model = tf.keras.models.Model(encoder_inputs, #Padded input sequences\n","                                      encoder_states) #Hidden state and Cell state at last time step"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l7hYyHfm1Gi1"},"source":["encoder_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jj07QPsc941I"},"source":["encoder_model.output"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"tBvy3uDnKXof"},"source":["### Build the Decoder Model\n","<p/>\n","\n","<ol><li>Define Input for both 'h' state and 'c' state initialization </li>\n","<li>Get Decoder RNN outputs along with h and c state</li>\n","<li>Get Decoder Dense layer output</li>\n","        <li>Build Model</li></ol>"]},{"cell_type":"markdown","metadata":{"id":"Ohsoa5J_KXof"},"source":["##### Step 1 - Define Input for both 'h' state and 'c' state initialization"]},{"cell_type":"code","metadata":{"id":"aNlm9ufvKXoh"},"source":["#Hidden state input\n","decoder_state_input_h = tf.keras.layers.Input(shape=(memory_size))\n","\n","#Cell state input\n","decoder_state_input_c = tf.keras.layers.Input(shape=(memory_size))\n","\n","#Putting it together\n","decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZzbTg44oKXoi"},"source":["##### Step 2 - Get Decoder RNN outputs along with h and c state"]},{"cell_type":"code","metadata":{"id":"OVHzmr1hKXoi"},"source":["#Get Embedding layer output\n","x = decoder_embedding(decoder_inputs)\n","\n","#We will use the layer which we trained earlier\n","rnn_outputs, state_h, state_c = decoder_rnn(x, initial_state=decoder_states_inputs)\n","\n","#Why do we need this?\n","decoder_states = [state_h, state_c]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1gX884-sKXoj"},"source":["##### Step 3 - Get Decoder Dense layer output"]},{"cell_type":"code","metadata":{"id":"lj9ZkJh0KXoj"},"source":["decoder_outputs = decoder_dense(rnn_outputs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0HiEPMqnKXok"},"source":["##### Step 4 - Build Decoder Model"]},{"cell_type":"code","metadata":{"id":"ExV0PWSPKXol"},"source":["decoder_model = tf.keras.models.Model([decoder_inputs] + decoder_states_inputs,  #Model inputs\n","                                      [decoder_outputs] + decoder_states)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7xFB-YGoCySr"},"source":["decoder_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qnKbxenGKXom"},"source":["# Predicting output from Seq2Seq model"]},{"cell_type":"markdown","metadata":{"id":"MFsy3KR7KXon"},"source":["##### Build a prediction function"]},{"cell_type":"code","metadata":{"id":"sIwVWxXXKXon"},"source":["decoder_t.word_index['<start>']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nR2H6c3YKXop"},"source":["int_to_word_decoder[51]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VEG8fmnbKXop"},"source":["def decode_sentence(input_sequence):\n","\n","    #Get the encoder state values - Sentence embedding\n","    decoder_initial_states_value = encoder_model.predict(input_seq)\n","\n","    #Build a sequence with '<start>' - starting sequence for Decoder\n","    target_seq = np.zeros((1,1))\n","    target_seq[0][0] = decoder_t.word_index['<start>']\n","\n","    #flag to check if prediction should be stopped\n","    stop_loop = False\n","\n","    #Initialize predicted sentence\n","    predicted_sentence = ''\n","\n","    num_of_predictions = 0\n","\n","    #start the loop\n","    while not stop_loop:\n","\n","        predicted_outputs, h, c = decoder_model.predict([target_seq] +\n","                                                        decoder_initial_states_value)\n","\n","        #Get the predicted word index with highest probability\n","        predicted_output = np.argmax(predicted_outputs[0,-1,:])\n","\n","        #Get the predicted word from predicter index\n","        predicted_word = int_to_word_decoder[predicted_output]\n","\n","        #Check if prediction should stop\n","        if(predicted_word == '<end>' or num_of_predictions > max_decoder_seq_length):\n","\n","            stop_loop = True\n","            continue\n","\n","        num_of_predictions += 1\n","\n","        #Updated predicted sentence\n","        if (len(predicted_sentence) == 0):\n","            predicted_sentence = predicted_word\n","        else:\n","            predicted_sentence = predicted_sentence + ' ' + predicted_word\n","\n","        #Update target_seq to be the predicted word index\n","        target_seq[0][0] = predicted_output\n","\n","        #Update initial states value for decoder\n","        decoder_initial_states_value = [h,c]\n","\n","\n","    return predicted_sentence"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_jtZM8iaKXoq"},"source":["##### Call Prediction function on a random sentence"]},{"cell_type":"code","metadata":{"id":"perGUle1KXor"},"source":["#Generate a random number\n","start_num = np.random.randint(0, high=len(encoder_text) - 10)\n","\n","#Predict model output for 5 sentences\n","for i in range(start_num, start_num + 5):\n","    input_seq = encoder_input_data[i : i+1]\n","    predicted_sentence = decode_sentence(input_seq)\n","    print('--------')\n","    print ('Input sentence: ', encoder_text[i])\n","    print ('Predicted sentence: ', predicted_sentence )"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xqi_-4Y0KXos"},"source":["##### Save encoder and decoder model"]},{"cell_type":"code","metadata":{"id":"1TbJoYhJKXos"},"source":["#Compile models to avoid error\n","encoder_model.compile(optimizer='adam',loss='categorical_crossentropy')\n","decoder_model.compile(optimizer='adam',loss='categorical_crossentropy')\n","\n","#Save the models\n","encoder_model.save('seq2seq_encoder_eng_hin.hd5')  #Encoder model\n","decoder_model.save('seq2seq_decoder_eng_hin.hd5')  #Decoder model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"skLA1matKXot"},"source":["##### Save encoder and decoder tokenizers"]},{"cell_type":"code","metadata":{"id":"eCLkBUL7KXou"},"source":["import pickle\n","\n","pickle.dump(encoder_t,open('encoder_tokenizer_eng','wb'))\n","pickle.dump(decoder_t,open('decoder_tokenizer_hin','wb'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7C04JeEpQs3w"},"source":["!ls -l"],"execution_count":null,"outputs":[]}]}